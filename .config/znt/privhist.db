kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
kubectl get --raw /apis/metrics.k8s.io/v1beta1
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
./hack/vpa-down.sh
k delete hr metrics-server
helm delete --purge metrics-server
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
kl -f vpa-recommender-57bfb768ff-v8xbb
./hack/vpa-up.sh
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f kube-system/metrics-server.yaml
helm upgrade -i prometheus-adapter stable/prometheus-adapter --set prometheus.url="http://prometheus.***REMOVED***.k8.***REMOVED***.com"
kubectl get helmrelease/prom-adapter -o yaml | yq .spec.values -y  | helm upgrade -i prom-adapter stable/prometheus-adapter -f -
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/prometheus-adapter -o yaml | yq .spec.values -y  | helm upgrade -i prom-adapter stable/prometheus-adapter -f -
helm delete --purge prometheus-adapter
kl -f vpa-updater-6dbf4d984c-n8tq4
kl -f vpa-recommender-57bfb768ff-jpdnz
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 3 -n kube-system
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
gc --amend
tmux attach -d -t 0
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
kl -f vpa-recommender-57bfb768ff-jpdnz
assume-role.sh 594505
kl -f vpa-updater-6dbf4d984c-n8tq4
k apply -f monitoring/elasticsearch-vpa.yaml
k describe vpa
k describe vpa elasticsearch-vpa
k apply -f external-secrets/
k describe vpa elasticsearch-vpa
tmux attach -d -t 0
k scale statefulset --replicas 2 elasticsearch-master
k scale statefulset --replicas 4 elasticsearch-master
kl -f vpa-updater-6dbf4d984c-n8tq4 -n kube-system
k apply -f monitoring/
k apply -f monitoring/
k apply -f kuberhealthy/
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
k scale statefulset --replicas 0 elasticsearch-master
k scale statefulset --replicas 4 elasticsearch-master
k scale statefulset --replicas 4 elasticsearch-master
k scale statefulset --replicas 0 elasticsearch-master
k scale statefulset --replicas 0 elasticsearch-master
k scale statefulset --replicas 4 elasticsearch-master
k rollout restart statefulset elasticsearch-master
k scale statefulset --replicas 0 elasticsearch-master
k scale statefulset --replicas 2 elasticsearch-master
tmux attach -d -t 0
assume-role.sh 133923
idea >/dev/null 2>&1 &
k edit statefulset elasticsearch-master
kdelp logstash-logstash-1 logstash-logstash-0
k edit statefulset logstash-logstash
k edit statefulset logstash-logstash
gc --amend --no-edit
gc --amend --no-edit
idea >/dev/null 2>&1 &
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
k edit statefulset logstash-logstash
k edit deploy
k apply -f monitoring/
kubectl get helmrelease/filebeat -o yaml | yq .spec.values -y  | helm upgrade -i filebeat elastic/filebeat -f - 
source ~/workspace/.env/bin/activate
kubectl get helmrelease/filebeat -o yaml | yq .spec.values -y  | helm upgrade -i filebeat elastic/filebeat -f - 
kubectl get helmrelease/kibana -o yaml | yq .spec.values -y  | helm upgrade -i kibana elastic/kibana -f -
gc -m "Updated helm operator charts."
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards chartmuseum/***REMOVED***-grafana-dashboards -f -
k apply -f monitoring/
k apply -f monitoring/
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards chartmuseum/***REMOVED***-grafana-dashboards -f -
kubectl get helmrelease/kibana -o yaml | yq .spec.values -y  | helm upgrade -i kibana elastic/kibana -f -
k apply -f monitoring/
kdhr kibana
kubectl get helmrelease/kibana -o yaml | yq .spec.values -y  | helm upgrade -i kibana elastic/kibana -f -
k delete hr kibana
k apply -f monitoring/
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="5m" \\
--set chartsSyncInterval="5m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
gc -m "Updated helm operator charts."
kdelp -n kube-system helm-operator-8fdc6c467-t9f4f
kdelp -n kube-system helm-operator-8fdc6c467-t9f4f --force --grace-period 0
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="4m" \\
--set chartsSyncInterval="4m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
kl -f helm-operator-78dcfcbdbf-47h8h
k apply -f monitoring/
kubectl get helmrelease/filebeat -o yaml | yq .spec.values -y  | helm upgrade -i filebeat elastic/filebeat -f - 
k delete vpa vmctl-northplains-xinet-vmcontroller-vpa
k apply -f monitoring/
kubectl get helmrelease/logstash -o yaml | yq .spec.values -y  | helm upgrade -i logstash elastic/logstash -f -
k apply -f monitoring/
kubectl get helmrelease/logstash -o yaml | yq .spec.values -y  | helm upgrade -i logstash elastic/logstash -f -
source ~/workspace/.env/bin/activate
gc --amend --no-edit
go build -tags=integration ./...
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
git clone git@github.com:***REMOVED***/gowrapper.git 
tmux attach -d -t 0
idea >/dev/null 2>&1 &
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
git rebase -i HEAD~4
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="4m" \\
--set chartsSyncInterval="4m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
source ~/workspace/.env/bin/activate
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="3m" \\
--set chartsSyncInterval="3m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
kdno ip-10-214-9-218.ec2.internal
k apply -f monitoring/
tmux attach -d -t 0
tmux
idea >/dev/null 2>&1 &
assume-role.sh 390019
k apply -f monitoring/
gc -m "Testing thanos"
watch -n 5 kubectl get helmrelease
k apply -f monitoring/
idea >/dev/null 2>&1 &
k apply -f monitoring/
k edit cm
k edit cm
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
kubectl -n monitoring create secret generic thanos-objstore-config --from-file=thanos.yaml=monitoring/thanos/secret.yaml
kubectl -n monitoring create secret generic thanos-objstore-config --from-file=thanos.yaml=monitoring/thanos/secret.yaml
k apply -f monitoring/
k apply -f monitoring/thanos
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh   
k apply -f monitoring/thanos
k apply -f kuberhealthy/
k apply -f kuberhealthy/
k apply -f monitoring/thanos
k apply -f monitoring/thanos
k delete pod thanos-store-0
k delete -f monitoring/thanos
k apply -f monitoring/
k delete -f monitoring/thanos
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
gc -m "Fixed typo"
k apply -f monitoring/
k apply -f monitoring/
kdelp prom-operator-grafana-6f64cdb988-4r8vv
k scale statefulset logstash-logstash --replicas 0
gc -m "Drop unwanted fields."
k apply -f monitoring/
k scale statefulset logstash-logstash --replicas 0
k apply -f monitoring/
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
tkn pr list -n synthetictx | grep -i succ | awk '{print $1}' | cat
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
k edit cm
kcn dcm
k edit deploy
kcn infinio
k apply -f monitoring/
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
k apply -f monitoring/
k apply -f monitoring/
k scale deployment tiller-deploy --replicas 0 -n kube-system
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="2m" \\
--set chartsSyncInterval="2m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
gc -m "Increased the alert tolerance for synthetic Txs"
k apply -f monitoring/
k scale deployment tiller-deploy --replicas 0 -n kube-system
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
source ~/workspace/.env/bin/activate
source ~/workspace/.env/bin/activate
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/
k apply -f monitoring/
source ~/workspace/.env/bin/activate
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh   
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh   
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh   
k apply -f monitoring/
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kl -f prometheus-prom-operator-prometheus-o-prometheus-0 thanos-sidecar
k apply -f monitoring/
gc -m "Testing"  
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/
gc -m "Testing"  
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/
k apply -f monitoring/thanos/
kubectl -n monitoring create secret generic thanos-objstore-config --from-file=thanos.yaml=/tmp/thanos-config.yaml
k apply -f monitoring/
kubectl get helmrelease/external-secrets -o yaml | yq .spec.values -y  | helm upgrade -i external-secrets chartmuseum/kubernetes-external-secrets -f - 
k apply -f external-secrets/
kgpvc | grep data | awk '{print $1}' | xargs kubectl delete pvc 
k delete -f monitoring/thanos/
k delete -f thanos/
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k delete -f thanos/
k apply -f thanos/
gc -m "Testing"  
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f thanos/
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f thanos/
k delete -f thanos/
k apply -f thanos/
k apply -f thanos/
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/
k apply -f thanos/
k apply -f thanos/
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
gc -m "Testing"  
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
k apply -f monitoring/
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
assume-role.sh 022370
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
helm3 upgrade -i thanos ./thanos 
helm create thanos
helm3 upgrade -i thanos ./thanos 
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
source ~/workspace/.env/bin/activate
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
./hack/vpa-down.sh
kdp thanos-querier-698dd7c68-q92lv
k apply -f monitoring/
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
gc -m "Bump up chart version"
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/thanos/
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k scale deployment tiller-deploy --replicas 0 -n kube-system
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
./hack/vpa-down.sh
./hack/vpa-down.sh
./hack/vpa-up.sh
kubectl apply -f deploy/vpa-beta-crd.yaml
./hack/vpa-down.sh
git clone git@github.com:***REMOVED***/***REMOVED***-infra-tests 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kdelp busybox
kubectl run -i --tty busybox --image=busybox --restart=Never 
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
gc -m "Added grpc port to prometheus service"
kubectl run -i --tty busybox --image=busybox --restart=Never 
kl -f thanos-querier-5f76b899bc-fdbdn
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
k scale deployment tiller-deploy --replicas 0 -n kube-system
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 0 -n kube-system
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
k scale deployment tiller-deploy --replicas 12 -n kube-system
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="4m" \\
--set chartsSyncInterval="4m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
k edit deploy -n impdh flux-impdh
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
gc --amend
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k scale deployment tiller-deploy --replicas 0 -n kube-system
k scale deployment tiller-deploy --replicas 12 -n kube-system
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="6m" \\
--set chartsSyncInterval="6m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/thanos.yaml
kgpvc | grep thanos | awk '{print $1}' | xargs kubectl delete pvc  
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k exec -it thanos-querier-5f76b899bc-tpkht /bin/sh
docker run --rm -it -v ./:/usr/share/logstash/pipeline/ docker.elastic.co/logstash/logstash:7.6.2
k apply -f kube-system/metrics-server.yaml
./hack/vpa-down.sh
k apply -f kube-system/metrics-server.yaml
kl -f metrics-server-76b749b7ff-mz92l
./hack/vpa-apply-upgrade.sh
./hack/vpa-down.sh
./hack/vpa-up.sh
k apply -f kube-system/metrics-server.yaml
k apply -f monitoring/prometheus-operator.yaml
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
k apply -f kube-system/metrics-server.yaml
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
./hack/vpa-apply-upgrade.sh
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
./hack/vpa-up.sh
sudo ./hack/vpa-down.sh
sudo ./hack/vpa-up.sh
k apply -f kube-system/metrics-server.yaml
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
assume-role.sh 873193
helm template ../../external_repos/helm_charts/stable/metrics-server/ -x templates/metrics-server-deployment.yaml | kubectl apply -f -
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm template metrics-server stable/metrics-server
k apply -f kube-system/metrics-server.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm template metrics-server stable/metrics-server
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm upgrade -i metrics-server stable/metrics-server -f -
kubectl get --raw /metrics
kubectl get --raw /apis/metrics.k8s.io/v1beta1
k exec -it thanos-querier-b9774ccd7-j7fds -- sh
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
k apply -f monitoring/thanos.yaml
k apply -f monitoring/thanos.yaml
k apply -f monitoring/
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/metrics-server -o yaml | yq .spec.values -y  | helm template metrics-server stable/metrics-server -f -
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/prometheus-operator.yaml
gc -m "Configure resources for thanos side car."
gc -m "Configure resources."
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/prometheus-operator.yaml
k scale deployment tiller-deploy --replicas 12 -n kube-system
k apply -f monitoring/prometheus-operator.yaml
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="6m" \\
--set chartsSyncInterval="6m" \\
--set resources.requests.cpu="1" \\
--set workers="40" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
gc -m "Bump up chart version"
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="6m" \\
--set chartsSyncInterval="6m" \\
--set resources.requests.cpu="1" \\
--set workers="40" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
gc -m "Testing whitespace commit"
gc -m "Testing whitespace commit"
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="3m" \\
--set chartsSyncInterval="3m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
gc --amend --no-edit
gc -m "Fixed typo"
k apply -f monitoring/
gc -m "Parse the output of helm-operator as json."
gc -m "Deleted outdated pr namespaces."
k apply -f monitoring/
gc -m "Fixed typo"
k apply -f monitoring/
gc -m "Split prometheus rules into a separate file"
k apply -f monitoring/
gc -m "Split prometheus alerts into a separate file"
k apply -f monitoring/
k edit deploy -n impdh flux-impdh
k apply -f monitoring/
gc -m "Fixed typo"
k apply -f monitoring/
k edit deploy -n impdh flux-impdh
gc -m "Parse the output of helm-operator as json."
k apply -f monitoring/
k apply -f monitoring/
gc -m "Split prometheus alerts into a separate file"
gc --amend --no-edit
k apply -f monitoring/
code ../../_misc-files/logstash.conf
k apply -f monitoring/
k apply -f monitoring/
k apply -f kuberhealthy/
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
k apply -f kuberhealthy/
startxfce4
lsb_release -a
idea >/dev/null 2>&1 &
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="3m" \\
--set chartsSyncInterval="3m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
k apply -f monitoring/
assume-role.sh 076051
assume-role.sh 076051
go build -v ./... 
go build -v 
./dhops helm-release -g something
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="5m" \\
--set chartsSyncInterval="5m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
./dhops helm-release -g something
k delete hr lol
kubectl apply -f kuberhealthy/lol.yaml
go build -v 
k delete hr lol
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="7m" \\
--set chartsSyncInterval="7m" \\
--set resources.requests.cpu="1" \\
--set workers="60" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
k delete hr lol
k delete hr lol
kubectl apply -f kuberhealthy/lol.yaml
k delete hr lol
kubectl apply -f kuberhealthy/lol.yaml
./dhops helm-release -g something
kcn synthetictx
k apply -f lol.yaml
k delete hr lol
k apply -f lol.yaml
go build -v 
./dhops helm-release -g something
k apply -f lol.yaml
k delete hr lol
./dhops helm-release -g something
k apply -f lol.yaml
k apply -f lol.yaml
go build -v 
k delete hr lol
k apply -f lol.yaml
k scale statefulset --replicas 0 logstash-logstash
k apply -f monitoring/prometheus-operator.yaml
gc -m "Fixed typo"
k apply -f monitoring/prometheus-operator.yaml
k apply -f monitoring/grafana-dashboards.yaml
gc -m "Bump up chart version"
k apply -f monitoring/grafana-dashboards.yaml
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set resources.requests.cpu="1" \\
--set workers="80" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
go build -v 
./dhops helm-release -g something
go build -v 
k delete hr lol
./dhops helm-release -g something
k apply -f lol.yaml
go build -v 
k delete hr lol
k apply -f lol.yaml
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
go build -v 
assume-role.sh 176154
fluxctl sync
go build -v 
./dhops helm-release -g something
go build -v 
fluxctl sync --k8s-fwd-ns synthetictx
go build -v 
./dhops helm-release -g something
go build -v 
./dhops helm-release -g something
git clone git@github.com:***REMOVED***/dhops
go build -v 
gc -m "Started abstracting away some of the helper functions."
./dhops helm-release -g something
go build -v 
helm completion zsh > ~/.oh-my-zsh/completions/_helm
source ~/.zshrc
go build -v 
./dhops helm-release -n synthetictx
go build -v 
k scale deploy --replicas 0 flux-synthetictx
kcn synthetictx
k scale deploy --replicas 0 flux-synthetictx
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
kcn impdh-***REMOVED***-pr-977
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-pr-977.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-javier.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
k apply -f kuberhealthy/
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="80" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
***REMOVED***-cli validate -f ./***REMOVED***-e2etester-import-mathapp-x.cicd-meta.yaml
***REMOVED***-cli validate -f ./***REMOVED***-e2etester-import-mathapp-x.cicd-meta.yaml
./synthetic-transaction validate
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit -x -f ./***REMOVED***-impdh-***REMOVED***.cicd-meta.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-shah.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-shah.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-shah.***REMOVED***.k8.***REMOVED***.com
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
gc --amend --no-edit
gc --amend --no-edit
kl ***REMOVED***-prod-slow-submit-yaml-1588810103 
./synthetic-transaction import -h
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=3e881617afc04052b6476d2e6d006538599f9ea7
export GITHUB_TOKEN=***REMOVED***
source ~/.zshrc
source ~/.zshrc
code ~/.oh-my-zsh/custom/profile.zsh
code ~/.oh-my-zsh/custom/profile.zsh
source ~/.zshrc
code ~/.oh-my-zsh/custom/profile.zsh
vi ~/.config/starship.toml
source ~/.zshrc
source ~/.zshrc
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
assume-role.sh 176154
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=3e881617afc04052b6476d2e6d006538599f9ea7
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-pr-977.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-pr-977.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-pr-977.***REMOVED***.k8.***REMOVED***.com
kl -f -n impdh-***REMOVED***-pr-977 ***REMOVED***-***REMOVED***-pr-977-7df4bd7499-jstxt
tkn pr list -n synthetictx | grep -i Failed | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
assume-role.sh 696885
gc -m "Fixed typo"
fluxctl sync --k8s-fwd-ns kube-system
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl identity --k8s-fwd-ns kube-system
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="80" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
assume-role.sh 176154
fluxctl sync --k8s-fwd-ns synthetictx 2>&1 > lol
kubectl completion zsh > ~/.oh-my-zsh/completions/_kubectl
source ~/.zshrc
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
source ~/workspace/.env/bin/activate
kubectl get helmrelease/thanos -o yaml | yq .spec.values -y  | helm upgrade -i thanos bitnami/thanos -f - 
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
pip upgrade
source ~/workspace/.env/bin/activate
python3 -m venv ~/workspace/.env
deactivate
sudo apt-get remove xdg-user-dirs --purge
sudo apt-get install python3-venv
idea >/dev/null 2>&1 &
python3 -m venv ~/workspace/.env
deactivate
source .env/bin/activate
fluxctl completion zsh > ~/.oh-my-zsh/completions/_fluxctl
source ~/.zshrc
fluxctl list-workloads --k8s-fwd-ns kube-system
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
glol importOrder.yaml
idea >/dev/null 2>&1 &
fluxctl sync --k8s-fwd-ns kube-system
fluxctl sync --k8s-fwd-ns kube-system
fluxctl
fluxctl sync --k8s-fwd-ns kube-system
kubectl completion zsh > ~/.oh-my-zsh/completions/_kubectl
source ~/.zshrc
k exec -it busybox-767bf65c49-4vg4w sh
kubectl run -i --tty busybox --image=busybox
kubectl run -i --tty busybox --image=busybox --restart=Never 
helm list --namespace monitoring
fluxctl sync --k8s-fwd-ns kube-system
assume-role.sh 154599
fluxctl sync --k8s-fwd-ns kube-system
fluxctl sync --k8s-fwd-ns kube-system
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
source ~/.zshrc
helm3 upgrade -i helm-operator fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="80" \\
--set logFormat="json" \\
--set resources.requests.memory=500Mi \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
k drain ip-10-214-9-205.ec2.internal --ignore-daemonsets --delete-local-data
tmux attach -d -t 0
k drain ip-10-214-9-205.ec2.internal --ignore-daemonsets --delete-local-data
k drain ip-10-214-9-218.ec2.internal --ignore-daemonsets --delete-local-data --force
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
fluxctl sync --k8s-fwd-ns kube-system
kcn kuberhealthy
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync --k8s-fwd-ns kube-system
assume-role.sh 671098
git clone git@github.com:***REMOVED***/dhops
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
kdelns podinfo
fluxctl sync --k8s-fwd-ns kube-system
git clone git@github.com:***REMOVED***/podinfo
***REMOVED***-cli startPipeline  -b master -c 14c0bb0 -p 2 -r podinfo
***REMOVED***-cli startPipeline  -b master -c 14c0bb0 -p 2 -r podinfo
fluxctl sync --k8s-fwd-ns impdh
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 89ae2ad
gc -m "Updated gitops dashboard."
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Dummy commit."
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c afa91b9
fluxctl sync --k8s-fwd-ns kube-system
k rollout restart deployment helm-operator
gc -m "Renamed service."
fluxctl sync --k8s-fwd-ns podinfo
kl -n kube-system -f helm-operator-6f878bfd77-r7pg8
gc -m "Bump up chart version"
fluxctl sync --k8s-fwd-ns podinfo
kl -n kube-system -f helm-operator-6f878bfd77-r7pg8
source ~/workspace/.env/bin/activate
kubectl get helmrelease/ingress-exteral -o yaml | yq .spec.values -y  | helm upgrade -i ingress-external stable/nginx-ingress -f -
gc -m "Testing"
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Bump up chart version"
fluxctl sync --k8s-fwd-ns podinfo
k exec -it ubuntu bash
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
fluxctl  --k8s-fwd-ns podinfo list-workloads
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 85d43f0
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
rm *.start
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 46c2439
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
gc -m "Experiment"
fluxctl list-workloads --k8s-fwd-ns podinfo
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 91bfd31
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
k describe khcheck ***REMOVED***-prod-slow-submit-yaml
tkn tasks describe ***REMOVED***-podinfo-c9d9f73
kubectl completion zsh > ~/.oh-my-zsh/completions/_kubectl
k api-resources
source ~/.zshrc
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli validate -f ./importOrder.yaml
k get secret flux-podinfo-git-deploy -o yaml
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
k exec -it ubuntu bash
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
kdelp ubuntu
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
tkn pr list -n synthetictx | grep -i Failed | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kl -f -n tekton-pipelines tekton-pipelines-controller-6bbf98846d-jzss2
gc -m "Testing"
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED***
kdelns podinfo podinfo-dev podinfo-staging podinfo-prod
kdelns podinfo podinfo-dev podinfo-staging podinfo-prod
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED*** -r
assume-role.sh 361633
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED*** -r
idea >/dev/null 2>&1 &
kcn impdh-***REMOVED***-pr-980
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED*** -r
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED*** -r
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
kcn impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \
: 1588952668:0;helm3 upgrade -i flux-podinfo fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \
: 1588952668:0;helm3 upgrade -i flux-podinfo fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true
k delete clusterrole flux-podinfo
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 4a52f26
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c a3b9a3e
fluxctl --k8s-fwd-ns podinfo identity
kdd -n kube-system flux-system
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo identity
fluxctl sync --k8s-fwd-ns podinfo
kdelns podinfo podinfo-dev podinfo-staging podinfo-prod
helm delete --purge podinfo-podinfo-pr-2
***REMOVED***-cli createWorkspace -p podinfo -t import-automation-tw -u ***REMOVED*** -r
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c d7c031f
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
helm3 upgrade -i flux-podinfo fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true
fluxctl --k8s-fwd-ns podinfo identity
k delete clusterrolebinding flux-podinfo
k delete clusterrole flux-podinfo
fluxctl sync --k8s-fwd-ns podinfo
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c d7c031f
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c d7c031f
kcn podinfo
kcn impdh-***REMOVED***-pr-980
kdelp ***REMOVED***-***REMOVED***-pr-980-859cf74489-fqhnc
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c d7c031f
kcn kube-system
kcn podinfo
kubectl run -i --tty ubuntu --image=ubuntu
echo 'apiVersion: batch/v1beta1\
kind: Job\
metadata:\
  labels:\
    chart: ***REMOVED***-e2e-tests-v0.1.0\
    draft: draft-app\
  name: ***REMOVED***-e2e-tests\
  namespace: e2etester\
spec:\
  concurrencyPolicy: Forbid\
  failedJobsHistoryLimit: 1\
  jobTemplate:\
    spec:\
      backoffLimit: 0\
      template:\
        metadata:\
        spec:\
          containers:\
            - envFrom:\
                - configMapRef:\
                    name: ***REMOVED***-e2e-tests\
                - secretRef:\
                    name: ***REMOVED***-e2e-tests\
              image: ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/***REMOVED***-e2e-tests:stg-97d6a06\
              imagePullPolicy: IfNotPresent\
              name: ***REMOVED***-e2e-tests\
              resources: {}\
              terminationMessagePath: /dev/termination-log\
              terminationMessagePolicy: File\
          dnsPolicy: ClusterFirst\
          restartPolicy: Never\
          schedulerName: default-scheduler\
          securityContext: {}\
          serviceAccount: xohub-sa\
          serviceAccountName: xohub-sa\
          terminationGracePeriodSeconds: 30' | kubectl apply -f -
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c d7c031f
curl -XPOST helm-operator:3030/api/v1/sync-git
curl -XPOST helm-operator:3030/api/v1/sync-git
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-images
fluxctl --k8s-fwd-ns podinfo list-images
gc -m "Increase replica count."
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
fluxctl --k8s-fwd-ns podinfo list-workloads
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
kubectl apply -f releases/dev/
fluxctl --k8s-fwd-ns podinfo list-workloads
gc -m "Automate"
kubectl apply -f releases/dev/
gc -m "Automate"
kubectl apply -f releases/dev/
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
***REMOVED***-cli startPipeline  -b master  -p 2 -r podinfo -c 05ab9f7 
fluxctl --k8s-fwd-ns podinfo list-workloads
fluxctl sync --k8s-fwd-ns podinfo
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 05ab9f7 
fluxctl --k8s-fwd-ns podinfo list-workloads
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn pr delete --force
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
kl -n podinfo -f flux-podinfo-68d857766d-4mkfl
fluxctl --k8s-fwd-ns podinfo list-workloads
fluxctl --k8s-fwd-ns tekton-pipelines list-workloads
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo list-workloads
kl -f flux-podinfo-68d857766d-gmqsv
fluxctl automate -w
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-images
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl list-images
kubectl apply -f releases/dev/
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-workloads
fluxctl automate -w
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl list-workloads
fluxctl list-images
kubectl apply -f releases/dev/
fluxctl list-images
fluxctl sync --k8s-fwd-ns podinfo
kubectl apply -f releases/dev/
fluxctl --k8s-fwd-ns podinfo release --all --update-image="***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd"
kubectl apply -f releases/dev/
fluxctl list-images
fluxctl --k8s-fwd-ns podinfo release --all
fluxctl --k8s-fwd-ns podinfo release --all --update-image="***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd"
gc -m "podinfo experiment"
k apply -f rbac/***REMOVED***-staging.yaml
fluxctl list-images
fluxctl --k8s-fwd-ns podinfo release --all --update-image="***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd"
fluxctl sync --k8s-fwd-ns podinfo
kl -f -n podinfo flux-podinfo-68d857766d-4mkfl
fluxctl list-images
fluxctl sync --k8s-fwd-ns podinfo
kl -f -n podinfo flux-podinfo-68d857766d-4mkfl
fluxctl list-workloads
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***/podinfo:master-a80d8bd
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-images
gc -m "Testing"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
fluxctl list-workloads
gc -m "Testing"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl sync --k8s-fwd-ns podinfo
fluxctl automate -w podinfo-dev:helmrelease/podinfo
fluxctl list-workloads
kl -f -n podinfo flux-podinfo-6fc679479-scgdn
kl -f -n podinfo flux-podinfo-6fc679479-scgdn
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
helm3 upgrade -i flux-podinfo fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-podinfo \\
--set registry.excludeImage="docker.io*" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true
fluxctl sync --k8s-fwd-ns podinfo
fluxctl sync --k8s-fwd-ns podinfo
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl list-workloads
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
***REMOVED***-cli startPipeline  -b master   -r podinfo -c a80d8bd
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl --k8s-fwd-ns podinfo release --all --update-all-images
fluxctl automate
fluxctl automate -w podinfo-dev:helmrelease/podinfo
fluxctl list-images
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-a80d8bd
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-1b5a199
fluxctl list-workloads
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-images
fluxctl release --all --update-all-images
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-1b5a199
k apply -f releases/***REMOVED***-staging.yaml
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-1b5a199
fluxctl automate -w podinfo-dev:helmrelease/podinfo
fluxctl automate -w podinfo-dev:helmrelease/podinfo
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-1b5a199
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 1b5a199
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 1b5a199
k apply -f releases/***REMOVED***-staging.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 47e87d6
k apply -f releases/***REMOVED***-staging.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 50354de
fluxctl identity --k8s-fwd-ns podinfo
fluxctl sync --k8s-fwd-ns podinfo
k edit secret -n kube-system git-ssh-secret
kdelp busybox
fluxctl sync --k8s-fwd-ns podinfo
k apply -f rbac/***REMOVED***-staging.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 50354de
k edit secret -n kube-system git-ssh-secret
fg
fluxctl list-images
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 454a633
make docker-push
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 62c9be7
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-workloads
fluxctl list-images
make docker-push
***REMOVED***-cli startPipeline  -b master   -r podinfo -c c2d396c
***REMOVED***-cli startPipeline  -b master   -r podinfo -p 1 -c c878e4d
***REMOVED***-cli startPipeline  -b ***REMOVED***-patch-1  -r podinfo -p 1 -c fc7301e
fluxctl list-images
assume-role.sh 036227
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
make docker-push
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b ***REMOVED***-patch-1  -r podinfo -p 1 -c 37f83f8
***REMOVED***-cli startPipeline  -b ***REMOVED***-patch-1  -r podinfo -p 1 -c 5815ff8
fluxctl list-images
fluxctl sync --k8s-fwd-ns podinfo
***REMOVED***-cli startPipeline  -b ***REMOVED***-patch-1  -r podinfo -p 1 -c d069668
***REMOVED***-cli startPipeline  -b master   -r podinfo -p 1 -c fc7301e
kgp -n impdh-***REMOVED***-pr-980
fluxctl list-images
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-567fb59476-r8mkb
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Automate"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-images --k8s-fwd-ns podinfo -n podinfo-staging
kubectl apply -f releases/dev/
fluxctl list-workloads
fluxctl list-images --k8s-fwd-ns podinfo -n podinfo-staging
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-images --k8s-fwd-ns podinfo 
fluxctl list-workloads
fluxctl sync --k8s-fwd-ns podinfo
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-567fb59476-26qpw
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 617d45b
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 0a46575
fluxctl list-images --k8s-fwd-ns podinfo 
kgp
kgp -n impdh-***REMOVED***-pr-980
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-567fb59476-ss4hd
rm *.start
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 4d04257
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 4d04257
fluxctl list-images --k8s-fwd-ns podinfo 
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 4d04257
fluxctl sync --k8s-fwd-ns podinfo
fluxctl list-images --k8s-fwd-ns podinfo 
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 4d04257
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl --k8s-fwd-ns podinfo release --all --update-image=***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo:master-1b5a199
fluxctl sync --k8s-fwd-ns podinfo
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl list-workloads
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-567fb59476-lgfmw
fluxctl list-images --k8s-fwd-ns podinfo 
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 44f9e55
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kgp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 7fbd152
fluxctl list-images --k8s-fwd-ns podinfo 
fluxctl --k8s-fwd-ns podinfo release --all --namespace podinfo-staging --update-all-images
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kgp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c 3e566c1
fluxctl --k8s-fwd-ns podinfo release --workload podinfo-staging:helmrelease/podinfo --namespace podinfo-staging --update-all-images
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kgp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c a66d764
gc -m "Automate"
fluxctl deautomate --workload podinfo-staging:helmrelease/podinfo
***REMOVED***-cli startPipeline  -b master   -r podinfo  -c e49b79f
fluxctl sync --k8s-fwd-ns podinfo
gc -m "Glob"
fluxctl sync --k8s-fwd-ns podinfo
fluxctl --k8s-fwd-ns podinfo release --workload podinfo-staging:helmrelease/podinfo --namespace podinfo-staging --update-all-images
***REMOVED***-cli startPipeline  -b master   -r podinfo -p 1 -c fc7301e
fluxctl list-images --k8s-fwd-ns podinfo 
kgp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -p 1 -c 
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kgp -n impdh-***REMOVED***-pr-980
gc -m "Glob"
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 64d2702
gc -m "Removed steps from dockerfile"
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 7353118
fluxctl --k8s-fwd-ns podinfo release --workload podinfo-pr-1:helmrelease/podinfo --namespace podinfo-staging --update-all-images
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c f57fc7a
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-567fb59476-fslnh
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 7353118
kgp -n impdh-***REMOVED***-pr-980
kdelp -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980-5cd99bd5b4-5bvtw
kubectl apply -f releases/dev/
kgp -n impdh-***REMOVED***-pr-980
kdp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c f0ced9a
***REMOVED***-cli startPipeline  -b master   -r podinfo -c f0ced9a
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl list-images --k8s-fwd-ns podinfo 
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
assume-role.sh 542107
source ~/.zshrc
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
code ~/.oh-my-zsh/custom/profile.zsh
source ~/.zshrc
source ~/.zshrc
vi ~/.zshrc
code ~/.oh-my-zsh/custom/profile.zsh
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
source ~/.zshrc
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-pr-977.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***-impdh-***REMOVED***-pr-977.***REMOVED***.k8.***REMOVED***.com
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
make build
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl --k8s-fwd-ns kube-system list-workloads -n kuberhealthy
k exec -it ubuntu-b7d6cb9c6-tl6rs bash
kcn kuberhealhty
fluxctl --k8s-fwd-ns kube-system list-images
kl -f flux-system-7d6c74fb89-nx6sh
gc -m "Parameterize image repository"
gc --amend --no-edit
fluxctl --k8s-fwd-ns kube-system list-images
gc -m "Bump up chart version"
fluxctl --k8s-fwd-ns kube-system list-workloads
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx -n kuberhealthy
gc -m "Bump up chart version"
fluxctl --k8s-fwd-ns kube-system list-workloads
fluxctl --k8s-fwd-ns kube-system list-images
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl --k8s-fwd-ns podinfo release --workload podinfo-staging:helmrelease/podinfo --namespace podinfo-staging --update-all-images
fluxctl --k8s-fwd-ns kube-system list-workloads
fluxctl sync --k8s-fwd-ns kube-system
fluxctl --k8s-fwd-ns kube-system list-images
gc -m "Fixed typo"
fluxctl sync --k8s-fwd-ns kube-system
fluxctl --k8s-fwd-ns kube-system list-images
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl --k8s-fwd-ns kube-system list-workloads
kl -n kube-system flux-system-f5bb97ff9-xlcgd
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl --k8s-fwd-ns podinfo release --all --namespace kuberhealthy --update-all-images
fluxctl list-images --k8s-fwd-ns podinfo 
fluxctl --k8s-fwd-ns podinfo release --all --namespace kuberhealthy --update-all-images
fluxctl sync --k8s-fwd-ns kube-system
fluxctl list-images --k8s-fwd-ns kube-system
k exec -it ubuntu-b7d6cb9c6-tl6rs bash
fluxctl --k8s-fwd-ns kube-system list-images
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl --k8s-fwd-ns kube-system list-images -n monitoring
export FLUX_FORWARD_NAMESPACE=podinfo
gc -m "Automate prometheus deployment"
fluxctl sync
fluxctl --k8s-fwd-ns kube-system automate -w monitoring:helmrelease/prom-operator -n monitoring
fluxctl sync
gc -m "Automate prometheus deployment"
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
gc -m "Testing"
gc -m "Automate prometheus deployment"
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc -m "Automate grafana deployment"
fluxctl sync
fluxctl --k8s-fwd-ns kube-system list-images -n monitoring
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc --amend -m "Testing"
fluxctl list-images
gc --amend -m "Testing"
gc --amend -m "Testing"
gc -m "Testing"
fluxctl sync
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl list-images
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-images
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-workloads
k apply -f releases/monitoring/prometheus-operator.yaml
kdhr prom-operator
fluxctl list-workloads
k apply -f releases/monitoring/prometheus-operator.yaml
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-workloads
k apply -f releases/monitoring/prometheus-operator.yaml
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:7.0.0-beta3
kdhr prom-operator
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:7.0.0-beta3
k apply -f releases/monitoring/prometheus-operator.yaml
fluxctl list-images
gc -m "Testing"
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:7.0.0-beta3
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl sync
fluxctl list-images
k apply -f releases/monitoring/prometheus-operator.yaml
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
kdhr prom-operator
k apply -f releases/monitoring/prometheus-operator.yaml
kdhr prom-operator
kgp -n kube-system
gc -m "Testing"
fluxctl sync
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-workloads
fluxctl list-images
k apply -f releases/monitoring/prometheus-operator.yaml
kdhr prom-operator
k apply -f releases/monitoring/prometheus-operator.yaml
kdhr prom-operator
fluxctl list-images
k apply -f releases/monitoring/prometheus-operator.yaml
kdhr prom-operator
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:7.0.0-beta3
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
kdhr prom-operator
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-workloads
kdelp busybox
fluxctl sync
fluxctl list-workloads
fluxctl list-images
fluxctl list-workloads
gc -m "Automate thanos"
fluxctl list-workloads
gc -m "Wait"
fluxctl sync
fluxctl list-workloads
fluxctl list-images
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
fluxctl sync
fluxctl list-workloads
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-images
gc -m "Automate thanos"
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
k apply -f releases/monitoring/prometheus-operator.yaml
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
k apply -f releases/monitoring/thanos.yaml
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
gapa
gc -m "Automate thanos"
fluxctl list-images
fluxctl sync
fluxctl list-workloads
fluxctl sync
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
export FLUX_FORWARD_NAMESPACE=kube-system
export FLUX_FORWARD_NAMESPACE=kuberhealthy
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-workloads 
fluxctl list-images
k apply -f releases/kuberhealthy/
fluxctl list-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
k apply -f releases/kuberhealthy/
gc -m "Testing"
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl list-images
k apply -f releases/kuberhealthy/
fluxctl list-images
fluxctl list-workloads
gc -m "Testing"
fluxctl sync
k edit deployment -n kube-system flux-system
kl -f -n kube-system flux-system-77878dd4-5g2vk
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl list-images
fluxctl list-images
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace kube-system \\
--set git.url=git@github.com:***REMOVED***/***REMOVED***-application-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage\[0\]="k8s.gcr.io/*" \\
--set automationInterval="3m" \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
k apply -f releases/kuberhealthy/
fluxctl list-images
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl list-images
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
fluxctl list-images
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl sync
kdhr kuberhealthy
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:6.7.3
fluxctl list-images
gc -m "Testing"
fluxctl list-workloads
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl list-images
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
gc -m "Testing"
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
fluxctl list-images
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
fluxctl list-workloads
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Testing"
gc -m "Testing"
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
gc -m "Automate prometheus deployment"
fluxctl sync
fluxctl list-workloads
gc -m "Wait"
fluxctl list-workloads
gc -m "Automate deployments"
fluxctl  release --workload monitoring:helmrelease/thanos --update-all-images
gc -m "Automate deployments"
gc -m "Automate deployments"
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-image grafana/grafana:6.7.3
fluxctl list-workloads
gc -m "Automate deployments"
fluxctl list-workloads
fluxctl  release --workload monitoring:helmrelease/logstash --update-all-images
gc -m "Automate deployments"
fluxctl sync
gc -m "Automate deployments"
fluxctl list-workloads
fluxctl list-workloads
fluxctl list-workloads
gc -m "Automate deployments"
gc --amend -m "Testing"
fluxctl sync
gc --amend -m "Automate deployments"
gc -m "Automate deployments"
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
fluxctl list-workloads
gc -m "Automate deployments"
fluxctl sync
fluxctl list-workloads
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
kl -f ***REMOVED***-prod-slow-submit-yaml-1589303794
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
alias
source ~/.zshrc
source ~/.zshrc
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl  release --workload monitoring:helmrelease/prom-operator --update-all-images
source ~/.zshrc
source ~/.zshrc
zshreload
zshreload
zshreload
zshreload
zshreload
zshreload
zshreload
source ~/.zshrc
zshreload
gco .
zshreload
zshreload
zshreload
zshconfig
alias
assume-role.sh 542107
zshreload
zshreload
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 8292a65
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
kgp -n impdh-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 791fe73
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 6ddc8d6
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c d107923
export FLUX_FORWARD_NAMESPACE=podinfo
ohmyzsh
fluxctl
zshreload
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 9335ca3
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ff0bece
kl -f -n podinfo flux-podinfo-7c6c57c57b-rk6vm
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 44f94f9
fluxctl sync
export FLUX_FORWARD_NAMESPACE=podinfo
gc -m "Disable automation"
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 3a3a85c
kcn podinfo-staging
k apply -f releases/monitoring/prometheus-operator.yaml
kubectl rollout restart --namespace monitoring statefulset prometheus-prom-operator-prometheus-o-prometheus
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
gc -m "Parameterize registry"
gc -m "Added datasource variable"
k rollout restart deployment prom-operator-grafana
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kdhr grafana-dashboards
fluxctl sync
source ~/workspace/.env/bin/activate
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards chartmuseum/***REMOVED***-grafana-dashboards -f -
kdhr grafana-dashboards
kdhr grafana-dashboards
kdhr grafana-dashboards
k rollout restart deployment prom-operator-grafana
k scale statefulset --replicas 0 thanos-storegateway
kl -f vpa-updater-57954c88b6-pddxp
gc -m "Set the replica back to 1."
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k apply -f releases/monitoring/thanos-vpa.yaml
k apply -f releases/monitoring/thanos-vpa.yaml
k apply -f releases/monitoring/thanos-vpa.yaml
gc -m "Set the replica count to 2."
fluxctl sync
gc -m "Updated dashboard"
fluxctl sync
kdhr grafana-dashboards
k rollout restart deployment prom-operator-grafana
kdhr grafana-dashboards
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
k rollout restart deployment prom-operator-grafana
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
gc -m "Configure resources."
fluxctl sync
gc -m "Set the replica count to 2."
gc -m "Set the replica count to 2."
fluxctl sync
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 3a3a85c
gc -m "Set the replica count to 2."
fluxctl sync
make docker-push
***REMOVED***-cli startPipeline  -b master   -r podinfo -c dbc5455
kubectl rollout restart statefulset --namespace monitoring logstash-logstash
k scale statefulset --replicas 1 thanos-storegateway
fluxctl sync
gc -m "Replica label"
fluxctl sync
gc -m "Replica label"
fluxctl sync
k apply -f releases/monitoring/thanos-vpa.yaml
gc -m "Wait"
k apply -f releases/monitoring
k apply -f releases/monitoring
fluxctl sync
gc -m "Wait"
fluxctl sync
gc -m "Replica label"
k apply -f releases/monitoring
k apply -f releases/monitoring
fluxctl sync
kdhr prom-operator
k apply -f releases/monitoring
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
kdhr prom-operator
gc -m "Replica label"
fluxctl sync
k scale deploy --replicas 1 flux-synthetictx
k scale deployment vpa-recommender --replicas 0
k apply -f releases/monitoring
gc -m "Set the replica back to 1."
gc -m "Set the replica back to 1."
fluxctl sync
k rollout restart deployment thanos-querier
gc -m "Updated dashboard"
k apply -f releases/monitoring
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
gc -m "Updated dashboard"
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
***REMOVED***-cli startPipeline  -b master   -r podinfo -c dbc5455
make docker-push
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
fluxctl completion zsh > ~/.oh-my-zsh/completions/_fluxctl
source ~/workspace/.env/bin/activate
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 419e65e
gc --amend --no-edit
docker build -f ***REMOVED***/Dockerfile .
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 419e65e
tkn pr list -n synthetictx | grep -i fail | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 98fff7a
make docker-push
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 98fff7a
k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c e322694 
kl -f flux-podinfo-67d5797c75-5ks5g
***REMOVED***-cli startPipeline  -b master   -r podinfo -c d7a03b9
k rollout restart deployment flux-podinfo
***REMOVED***-cli startPipeline  -b master   -r podinfo -c d7a03b9
kl -f flux-podinfo-79bbdb4f45-5cw2p
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 6e44279
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 2754234
assume-role.sh 318349
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
source ~/workspace/.env/bin/activate
gc --amend --no-edit
docker build -f ***REMOVED***/Dockerfile .
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 1b385d0
gc --amend --no-edit
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 6cef0b2
gc --amend --no-edit
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 6cef0b2
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
kdhr prom-operator
helm completion
tkn completion zsh > ~/.oh-my-zsh/completions/_tkn
zshreload
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
source ~/workspace/.env/bin/activate
ohmyzsh
zshreload
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 3591306
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
***REMOVED***-cli validate -f ./importOrder.yaml
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
assume-role.sh 301925
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c 3591306
gco ***REMOVED***-experiment
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
vi ~/.***REMOVED***-cli.yaml
git clone git@github.com:tektoncd/cli.git
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-980 ***REMOVED***-***REMOVED***-pr-980
***REMOVED***-cli validate -f ./importOrder.yaml
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-990 ***REMOVED***-***REMOVED***-pr-990
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-990 ***REMOVED***-***REMOVED***-pr-990
k apply -f releases/monitoring/logstash.yaml
kcn impdh-***REMOVED***-pr-990
k apply -f servicemonitor.yaml
k apply -f servicemonitor.yaml
kubectl rollout restart --namespace monitoring statefulset prometheus-prom-operator-prometheus-o-prometheus 
kl -f prometheus-prom-operator-prometheus-o-prometheus-0 prometheus
kl -f prometheus-prom-operator-prometheus-o-prometheus-0 prometheus
kcn impdh-***REMOVED***-pr-990
k apply -f servicemonitor.yaml
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k apply -f servicemonitor.yaml
kcn impdh-***REMOVED***-pr-990
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k apply -f servicemonitor.yaml
k apply -f servicemonitor.yaml
k apply -f servicemonitor.yaml
fluxctl sync
kubectl rollout restart --namespace monitoring statefulset prometheus-prom-operator-prometheus-o-prometheus 
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-990 ***REMOVED***-***REMOVED***-pr-990
k apply -f .
idea >/dev/null 2>&1 &
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-workloads
fluxctl list-images -w podinfo-staging:helmrelease/podinfo| grep master-1cbf0xxxd | wc -l
fluxctl  release --workload monitoring:helmrelease/kibana --update-all-images
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl sync
k apply -f releases/monitoring/logstash.yaml
fluxctl sync
***REMOVED***-cli validate -f ./importOrder.yaml
k rollout restart deployment flux-podinfo
fluxctl list-workloads
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kl ***REMOVED***-prod-fast-validate-yaml-1589395523
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kl ***REMOVED***-prod-fast-validate-yaml-1589395523
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kl -f ***REMOVED***-prod-slow-submit-yaml-1589396088
assume-role.sh 311337
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
gco -b cleanup-garbagecollector-logs
gc -m "Set the trace level for the garbage collector to DEBUG."
gc --amend --no-edit
gc --amend --no-edit
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
source assume-role.sh 387396
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-990 ***REMOVED***-***REMOVED***-pr-990
idea >/dev/null 2>&1 &
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-993 ***REMOVED***-***REMOVED***-pr-993
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli validate -f ./importOrder.yaml
make docker-push && k rollout restart deployment -n impdh-***REMOVED***-pr-993 ***REMOVED***-***REMOVED***-pr-993
kl -f ***REMOVED***-***REMOVED***-pr-993-789697d44-rwbkw
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
curl -X POST -g 'http://prometheus.***REMOVED***.k8.***REMOVED***.com/api/v1/admin/tsdb/delete_series?match[]={persistentvolumeclaim=~".*elasticsearch.*"}'
curl -X POST -g 'http://prometheus.***REMOVED***.k8.***REMOVED***.com/api/v1/admin/tsdb/delete_series?match[]=kubernetes_build_info'
curl -X POST -g 'http://prometheus.***REMOVED***.k8.***REMOVED***.com/api/v1/admin/tsdb/delete_series?match[]=kubernetes_build_info'
gc --amend --no-edit
fluxctl sync
gc --amend --no-edit
gc --amend --no-edit
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
ohmyzsh
kubectl completion zsh > ~/.oh-my-zsh/completions/_kubectl
zshreload
vi ~/.config/starship.toml
ohmyzsh
zshreload
zshreload
idea >/dev/null 2>&1 &
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
kgp
fluxctl sync
k apply -f releases/kuberhealthy/
k apply -f releases/kuberhealthy/
k apply -f releases/kuberhealthy/
source ~/workspace/.env/bin/activate
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/kuberhealthy -f -
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/kuberhealthy -f -
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/kuberhealthy -f -
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl sync
fluxctl sync
gc -m "Fixed hardcoded check name"
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
idea >/dev/null 2>&1 &
assume-role.sh 657204
assume-role.sh 657204
k scale deployment --replicas 0 helm-operator-sonar
source ~/workspace/.env/bin/activate
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y   | helm template   ../***REMOVED***-synthetic-transactions/deploy/***REMOVED***-synthetic-tx -x templates/validate-order.yaml
kubectl get helmrelease/filebeat -o yaml | yq .spec.values -y  | helm upgrade -i filebeat elastic/filebeat -f - 
k scale deployment --replicas 0 helm-operator-sonar
k scale deployment --replicas 0 helm-operator-system
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
curl -X POST -g 'http://prometheus.***REMOVED***.k8.***REMOVED***.com/api/v1/admin/tsdb/delete_series?match[]=***REMOVED***_api_startpipeline_count'
curl -X POST -g 'http://prometheus.***REMOVED***.k8.***REMOVED***.com/api/v1/admin/tsdb/clean_tombstones'
zshreload
zshreload
idea >/dev/null 2>&1 &
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl sync
fluxctl sync
fluxctl sync
./hack/vpa-apply-upgrade.sh
kubectl create job --from=cronjob/descheduler descheduler-manual-001
kl -f descheduler-manual-001-ssrpx
fluxctl sync
k rollout restart deployment kuberhealthy 
fluxctl sync
fluxctl sync
assume-role.sh 004372
fluxctl sync
assume-role.sh 775994
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
source ~/workspace/.env/bin/activate
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kcn impdh-***REMOVED***-pr-980
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kcn impdh-***REMOVED***-pr-980
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
kcn impdh-***REMOVED***-pr-980
gco ***REMOVED***-metrics
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
idea >/dev/null 2>&1 &
gc -m "Add request result label to the API metrics."
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
idea >/dev/null 2>&1 &
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
idea >/dev/null 2>&1 &
assume-role.sh 750627
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl sync
export FLUX_FORWARD_NAMESPACE=podinfo
fluxctl sync
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
fluxctl sync
assume-role.sh 491074
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
helm rollback elasticsearch 20
idea >/dev/null 2>&1 &
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl
helm3 upgrade -i helm-operator-kube-system fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="kube-system" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
fluxctl sync
fluxctl sync
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
gc --amend --no-edit
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
fluxctl sync
fluxctl sync
kubectl get helmrelease/elasticsearch -o yaml | yq .spec.values -y  | helm upgrade -i elasticsearch elastic/elasticsearch -f -
kubectl get helmrelease/elasticsearch -o yaml | yq .spec.values -y  | helm upgrade -i elasticsearch elastic/elasticsearch -f -
helm list --namespace monitoring
kubectl get helmrelease/elasticsearch -o yaml | yq .spec.values -y  | helm upgrade -i elasticsearch elastic/elasticsearch -f -
kubectl get helmrelease/elasticsearch -o yaml | yq .spec.values -y  | helm upgrade -i elasticsearch elastic/elasticsearch -f -
kl -f vpa-updater-76f446d97c-rs2kd
fluxctl sync
kl -f vpa-updater-76f446d97c-rs2kd -n kube-system
fluxctl sync
kl -f vpa-updater-76f446d97c-rs2kd -n kube-system
gc --amend -m "Automate deployments"
kl -f vpa-updater-76f446d97c-rs2kd -n kube-system
fluxctl sync
kl -f vpa-updater-76f446d97c-rs2kd -n kube-system
***REMOVED***-cli validate -f ./importOrder-invalidTest.yaml
vi ~/.***REMOVED***-cli.yaml
gc -m "Inject toggle repository,"
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx -n kuberhealthy
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/kuberhealthy
fluxctl list-workloads
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl list-workloads
fluxctl sync
fluxctl list-workloads
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx
fluxctl list-workloads
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-image kuberhealthy/pod-restarts-check:v2.1.1
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl sync
fluxctl list-images
fluxctl sync
fluxctl list-workloads
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl list-images
fluxctl sync
fluxctl list-workloads
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl list-images
gc -m "Release the latest version of KH"
fluxctl sync
fluxctl list-workloads
gc -m "Release the latest version of KH"
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl list-images
gc -m "Release the latest version of KH"
fluxctl sync
fluxctl list-workloads
fluxctl list-images
gc -m "Release the latest version of KH"
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
fluxctl sync
k edit khcheck ***REMOVED***-production-slow-submit-yaml
k edit khcheck ***REMOVED***-staging-slow-submit-yaml
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-all-images
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx
fluxctl list-images
fluxctl list-workloads
fluxctl --k8s-fwd-ns kube-system automate -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx
kl -f kuberhealthy-cd4db9668-mf5ff
kl ***REMOVED***-staging-slow-submit-yaml-1589585539
fluxctl  release --workload kuberhealthy:helmrelease/kuberhealthy --update-image kuberhealthy/pod-restarts-check:v2.1.1
fluxctl list-images
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.5
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-all-images
fluxctl list-images
k edit khcheck ***REMOVED***-staging-slow-submit-yaml
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k edit khcheck ***REMOVED***-staging-slow-submit-yaml
k edit khcheck ***REMOVED***-production-slow-submit-yaml
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit --no-verify
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl sync
fluxctl sync
fluxctl sync
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
***REMOVED***-cli submit  -x -f ./importOrder-invalidTest.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
vi ~/.***REMOVED***-cli.yaml
tkn pr list -n synthetictx | grep -i fail | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
kcn impdh-***REMOVED***-pr-999
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder-invalidTest.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
export DOCKER_TAG=debug-metrics
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
make docker-push
k edit deploy
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
kl -f ***REMOVED***-staging-795bcdf656-n9s7p
kcn impdh-staging
k edit deploy
kl -f ***REMOVED***-staging-795bcdf656-n9s7p
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl sync
fluxctl list-workloads
fluxctl list-images
***REMOVED***-cli validate -f ./importOrder.yaml
kl -f ***REMOVED***-staging-795bcdf656-n9s7p
***REMOVED***-cli validate -f ./importOrder.yaml
kubectl config use-context dev-cluster
kubectl config use-context dev-cluster
kubectl config use-context STAGING
git clone git@github.com:***REMOVED***/***REMOVED***-feature-toggles.git 
fluxctl list-workloads
fluxctl list-images
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
kgp -n impdh-***REMOVED***-pr-999
fluxctl list-images
k apply -f impdh-gitops/releases/***REMOVED***-pr-999/
k apply -f impdh-gitops/releases/***REMOVED***-pr-999/
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
vi ~/.***REMOVED***-cli.yaml
gc --amend --no-edit --no-verify
fluxctl list-images
fluxctl sync
fluxctl list-images
fluxctl list-images
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl list-images
fluxctl list-workloads
k apply -f releases/feature-toggles/
k delete pvc data-unleash-postgresql-master-0 data-unleash-postgresql-slave-0 data-unleash-postgresql-slave-1
k get secret -o yaml unleash-postgresql
echo 'WW1NT0o5amlIUw==' | base64 -d 
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
kcn feature-toggles
kcn impdh-***REMOVED***-pr-999
kcn impdh-staging
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
kl -f ***REMOVED***-staging-98b6dbc8b-8t64n
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
gc --amend --no-edit
vi ~/.***REMOVED***-cli.yaml
vi ~/.***REMOVED***-cli.yaml
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.8
fluxctl list-workloads
fluxctl list-images
fluxctl list-workloads
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-unleash:0.1.2
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-workloads --namespace kuberhealthy
vi ~/.***REMOVED***-cli.yaml
k delete pvc data-unleash-postgresql-master-0 data-unleash-postgresql-slave-0 data-unleash-postgresql-slave-1
k apply -f releases/feature-toggles/
k apply -f releases/feature-toggles/
k apply -f releases/feature-toggles/
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
kcn feature-toggles
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
assume-role.sh 045803
fluxctl sync
tkn pr list -n synthetictx | grep -i succ | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
k apply -f releases/feature-toggles/
fluxctl sync
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
fluxctl sync
fluxctl sync
k delete pvc data-unleash-postgresql-master-0 data-unleash-postgresql-slave-0 data-unleash-postgresql-slave-1
k apply -f releases/feature-toggles/
k get secrest
k get secret
fluxctl sync
k apply -f releases/feature-toggles/
gc --amend --no-edit
fluxctl sync
fluxctl sync
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
k rollout restart deployment unleash
***REMOVED***-cli validate -f ./importOrder.yaml
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
gc --amend --no-edit
gc --amend --no-edit
DOCKER_TAG=debug-1 make docker-push
gc -m "Implemented EnvironmentActivationStrategy"
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
fluxctl sync
DOCKER_TAG=debug-1 make docker-push
helm3 upgrade -i helm-operator-pr-1000 fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="impdh-***REMOVED***-pr-1000" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
fluxctl sync
***REMOVED***-cli validate -f ./importOrder.yaml
DOCKER_TAG=debug-1 make docker-push
DOCKER_TAG=debug-1 make docker-push
***REMOVED***-cli validate -f ./importOrder.yaml
kdelp ***REMOVED***-***REMOVED***-pr-1000-5c77b7ff8d-9wgcg
fluxctl sync
kl -f ***REMOVED***-***REMOVED***-pr-1000-659c5df645-lw8qc
***REMOVED***-cli validate -f ./importOrder.yaml
DOCKER_TAG=debug-1 make docker-push
kdelp ***REMOVED***-***REMOVED***-pr-1000-659c5df645-lw8qc
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
kdelp ***REMOVED***-***REMOVED***-pr-1000-659c5df645-lw8qc
***REMOVED***-cli validate -f ./importOrder.yaml
kdelp ***REMOVED***-***REMOVED***-pr-1000-57fbc97f87-r2hr4
***REMOVED***-cli validate -f ./importOrder.yaml
DOCKER_TAG=debug-1 make docker-push
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli validate -f ./importOrder.yaml
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
kubectl completion zsh > ~/.oh-my-zsh/completions/_kubectl
zshreload
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
sudo vi /etc/postgresql/12/main/pg_hba.conf
sudo service postgresql restart
node server.js
node server.js
node server.js
fluxctl list-workloads
fluxctl list-images
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-unleash:0.1.2
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-workloads
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-unleash:0.1.2
gc --amend --no-edit
gc --amend --no-edit
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-images
echo 'node_modules' > .gitignore
node server.js
assume-role.sh 045803
node server.js
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
fluxctl list-images
fluxctl sync
k apply -f releases/feature-toggles/
fluxctl sync
fluxctl sync
node server.js
assume-role.sh 045803
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
kl -f ***REMOVED***-***REMOVED***-pr-996-7bcff5c797-gqkwc
***REMOVED***-cli validate -f ./importOrder.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli validate -f ./importOrder.yaml
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli validate -f ./importOrder-invalidTest.yaml
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED***-calculator -c d5ddd2d
tmux
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
idea >/dev/null 2>&1 &
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.9
export FLUX_FORWARD_NAMESPACE=kuberhealthy
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.10
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
assume-role.sh 456705
fluxctl list-workloads
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
kcn synthetictx
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
zshreload
eval "$(starship init zsh)"
curl -fsSL https://starship.rs/install.sh | bash
eval "$(starship init zsh)"
vi ~/.config/starship.toml
zshreload
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
idea >/dev/null 2>&1 &
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
docker rm etcd
etcdctl put foo bar
etcdctl get foo
etcdctl put foo3 lolololol
etcdctl put foo3 lolololol
docker run -it  -p 2379:2379 --rm -e ALLOW_NONE_AUTHENTICATION=yes --name etcd bitnami/etcd
docker run -it -e ALLOW_NONE_AUTHENTICATION=yes --name etcd bitnami/etcd
fluxctl sync
etcdctl put foo lol
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
k apply -f releases/etcd/
fluxctl sync
fluxctl list-images
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl list-images
k apply -f releases/etcd/
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.11
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k exec -it ubuntu-b7d6cb9c6-tl6rs bash -n podinfo
k get secret
zshreload
k apply -f releases/etcd/
fluxctl sync
k apply -f releases/etcd/
k apply -f releases/etcd/
k apply -f releases/etcd/
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-tl6rs zsh -n podinfo
k get secret etcd -o yaml
fluxctl sync
fluxctl list-images
fluxctl sync
fluxctl sync
fluxctl list-images
fluxctl  release --workload etcd:helmrelease/etcd --update-all-images
fluxctl  release --workload etcd:helmrelease/etcd --update-all-images
fluxctl list-images
fluxctl  release --workload etcd:helmrelease/etcd --update-all-images
fluxctl sync
fluxctl  release --workload etcd:helmrelease/etcd --update-all-images
fluxctl sync
kdelpvc data-etcd-0
cd ***REMOVED***-application-gitops
k apply -f releases/etcd/
fluxctl sync
k apply -f releases/etcd/
fluxctl sync
k apply -f releases/etcd/
fluxctl sync
k get secret -o yaml etcd
echo 'OE4zUjJ5bExOaQ=='  | base64 -d
kdelpvc data-etcd-0 data-etcd-1 data-etcd-2
k apply -f releases/etcd/
fluxctl sync
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
k exec -it ubuntu-b7d6cb9c6-tl6rs zsh -n podinfo
kubectl run -i --tty ubuntu --image=ubuntu
k exec -it ubuntu-b7d6cb9c6-tl6rs zsh -n podinfo
code ~/.config/starship.toml
zshreload
eval "$(starship init zsh)"
zshreload
zshconfig
zshreload
zshreload
zshreload
k exec -it ubuntu-b7d6cb9c6-x22wr zsh
idea >/dev/null 2>&1 &
fluxctl  release --workload etcd:helmrelease/etcd --update-image docker.io/bitnami/etcd:3.4.1
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-images
fluxctl  release --workload etcd:helmrelease/etcd --update-image docker.io/bitnami/etcd:3.4.8
code ~/.config/starship.toml
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl list-images
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl  release --workload etcd:helmrelease/etcd --update-image docker.io/bitnami/etcd:3.4.8
fluxctl sync
fluxctl sync
k apply -f releases/etcd/
kl -f etcd-0
k exec -it etcd-0 sh
kcn etcd
k exec -it ubuntu-b7d6cb9c6-x22wr zsh
fluxctl list-images
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl list-images
fluxctl list-images
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.11
fluxctl list-images
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
k exec -it etcd-0 sh
fluxctl sync
kdelpvc data-etcd-0 data-etcd-1 data-etcd-2
k apply -f releases/etcd/
fluxctl list-images
fluxctl sync
fluxctl sync
fluxctl list-workloads
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace kube-system \\
--set git.url=git@github.com:***REMOVED***/***REMOVED***-application-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="k8s.gcr.io/*\,gcr.io/tekton-releases/*\,docker.io/bitnami/postgresql:*" \\
--set registry.include="gcr.io/google-containers/*\,k8s.gcr.io/etcd-amd64" \\
--set registry.automationInterval="5m" \\
--set git.pollInterval=1m  \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
fluxctl sync
fluxctl list-images
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
fluxctl sync
kcn etcd
fluxctl sync
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
fluxctl sync
kcn etcd
etcdctl endpoint status --endpoints=http://etcd.***REMOVED***.k8.***REMOVED***.com
docker run -it -e ALLOW_NONE_AUTHENTICATION=yes --name etcd bitnami/etcd
fluxctl sync
etcdctl endpoint status --endpoints=127.0.0.1:80
gc -m "Installed lefthook"
gc --amend -m "Testing ingress"
gc --amend --no-edit
gc -m "Installed lefthook"
gc -m "Testing"
git reset HEAD~
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
assume-role.sh 868250
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
cd ***REMOVED***-application-gitops
k apply -f releases/etcd/
fluxctl sync
fluxctl list-images
etcdctl put foo lol
wget https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz
wget https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz
tar -zxvf go1.14.2.linux-amd64.tar.gz
tmux attach -d -t 0
wget https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz
tar -zxvf go1.14.2.linux-amd64.tar.gz
idea >/dev/null 2>&1 &
tar -zxvf go1.14.2.linux-amd64.tar.gz
idea >/dev/null 2>&1 &
sudo mv go /usr/local/
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
sudo apt-get install docker-ce-cli
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
git rebase master
git rebase HEAD~5 -i
gc --amend --no-edit
gc --amend --no-edit
idea >/dev/null 2>&1 &
vi ~/.***REMOVED***-cli.yaml
k scale deployment --replicas 0
./prometheus-exporter -i ***REMOVED***-impdh-***REMOVED***-pr-980
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
k scale deployment ***REMOVED***-***REMOVED***-pr-980 --replicas 0
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
k scale deployment ***REMOVED***-***REMOVED***-pr-980 --replicas 0
gc -m "Replaced local filesystem with etcd"
fluxctl sync
cd ***REMOVED***-prometheus-exporter
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
gc -m "Test"
gc -m "Submit to sonar from master only."
gc --amend --no-edit
gc --amend --no-edit
cat /etc/wsl.conf
fluxctl sync
k apply -f releases/feature-toggles/
k apply -f releases/feature-toggles/
fluxctl sync
docker run -it -p 80:2379 --rm -e ALLOW_NONE_AUTHENTICATION=yes --name etcd bitnami/etcd
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl  release --workload kuberhealthy:helmrelease/***REMOVED***-synthetic-tx --update-image ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-synthetic-transactions:0.7.14
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-workloadshelm3 upgrade -i flux-system fluxcd/flux \\
--namespace kube-system \\
--set git.url=git@github.com:***REMOVED***/***REMOVED***-application-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="k8s.gcr.io/*\,gcr.io/tekton-releases/*\,docker.io/bitnami/postgresql:*" \\
--set registry.include="gcr.io/google-containers/*" \\
--set registry.automationInterval="5m" \\
--set git.pollInterval=1m  \\
--set resources.requests.cpu=1000m \\
--set resources.requests.memory=500Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
fluxctl list-workloads
fluxctl list-images
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
k apply -f releases/feature-toggles/
docker run -it --rm --name unl ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***-unleash
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-images
make push
fluxctl sync
fluxctl list-images
fluxctl list-workloads
fluxctl sync
fluxctl sync
fluxctl sync
docker exec -it unl bash
assume-role.sh 610056
fluxctl sync
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl list-workloads
kubectl config use-context dev-cluster
fluxctl list-workloads
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-images
fluxctl sync
kubectl config use-context STAGING
fluxctl list-images
fluxctl sync
fluxctl sync
fluxctl sync
idea >/dev/null 2>&1 &
fluxctl sync
assume-role.sh 279944
kubectl config use-context dev-cluster
fluxctl list-images
fluxctl sync
k apply -f releases/feature-toggles/
k delete hr unleash
k delete pvc data-unleash-postgresql-postgresql-master-0
k apply -f releases/feature-toggles/
fluxctl sync
kubectl config use-context STAGING
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
docker build -f ./builder-go/Dockerfile-trusty-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:javier builder-go
kubectl config use-context dev-cluster
fluxctl sync
kubectl config use-context STAGING
sudo npm install -g aws-cdk
npm run watch
git clone git@github.com:***REMOVED***/***REMOVED***-infra-tests 
source ~/workspace/.env/bin/activate
pip install -r requirements.txt
pip install -r requirements.txt
gc --amend --no-edit
gc --amend --no-edit
gc --amend --no-edit
gc -m "Abort the commit if wire changes are required."
gc -m "Abort the commit if wire changes are required."
gc -m "Demo"
kcn impdh-***REMOVED***-pr-1011
idea >/dev/null 2>&1 &
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
assume-role.sh 970357
k apply -f releases/***REMOVED***-pr-1011/
tmux
tmux
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
k apply -f releases/***REMOVED***-pr-1011/
source ~/workspace/.env/bin/activate
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
k scale deploy --replicas 2
k scale deployment ***REMOVED***-***REMOVED***-pr-1011 --replicas 2 
k scale deployment ***REMOVED***-***REMOVED***-pr-1011 --replicas 1
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-1011 ./charts/***REMOVED*** -f - 
k apply -f releases/***REMOVED***-pr-1011/
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-1011 ./charts/***REMOVED*** -f - 
k edit deploy ***REMOVED***-***REMOVED***-pr-1011
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
k edit deploy ***REMOVED***-***REMOVED***-pr-1011
kcn impdh-***REMOVED***-pr-1011
./prometheus-exporter -e 127.0.0.1:2379 -i 'up{service="***REMOVED***",namespace="impdh-***REMOVED***-pr-1011"} < 0' -v
k edit deploy ***REMOVED***-***REMOVED***-pr-1011
k edit deploy ***REMOVED***-***REMOVED***-pr-1011
./prometheus-exporter -e 127.0.0.1:2379 -i 'min(up{service="***REMOVED***",namespace="impdh-***REMOVED***-pr-1011"})' -v
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
kgp -n external-secrets
fluxctl sync
code ~/.oh-my-zsh/custom/profile.zsh
zshconfig
zshreload
fluxctl sync
fluxctl list-workloads
fluxctl sync
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl sync
fluxctl list-images
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-images
zshconfig
zshreload
fluxctl sync
k exec -it ubuntu-b7d6cb9c6-x22wr zsh -n etcd
k exec -it ubuntu-b7d6cb9c6-tl6rs zsh -n podinfo
fluxctl sync
fluxctl list-images
fluxctl  release --workload feature-toggles:helmrelease/unleash --update-all-images
fluxctl list-images
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-rs2kd
echo 'VDJ6eHlyb2xXVQ==' | base64 -d
code ~/.oh-my-zsh/custom/profile.zsh
gc --amend --no-edit
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
k apply -f releases/etcd/
k apply -f releases/etcd/
fluxctl sync
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
gc --amend --no-edit
gc --amend --no-edit
fluxctl list-images
fluxctl  release --workload external-secrets:helmrelease/external-secrets --update-all-images
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
./prometheus-exporter -e 127.0.0.1:2379 -i 'min(up{service="***REMOVED***",namespace="impdh-***REMOVED***-pr-1011"})' -v
fluxctl sync
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
k exec -it -n podinfo ubuntu-b7d6cb9c6-8xgmx zsh
fluxctl sync
fluxctl sync
./prometheus-exporter -e 127.0.0.1:2379 -i 'min(up{service="***REMOVED***",namespace="impdh-***REMOVED***-pr-1011"})' -v --root-password ng12
git config --list --show-origin
fluxctl list-workloads 
cd ***REMOVED***-prometheus-exporter
export FLUX_FORWARD_NAMESPACE=kuberhealthy
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-workloads --container ***REMOVED***
k apply -f releases/monitoring/scrapers.yaml
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
fluxctl list-images --workload monitoring:helmrelease/***REMOVED***-prometheus-exporter
fluxctl sync
while [ $(fluxctl list-images -w monitoring:helmrelease/***REMOVED***-prometheus-exporter| grep 0.3.7| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 10; done;
k apply -f releases/etcd/
k exec -it -n podinfo ubuntu-b7d6cb9c6-8xgmx zsh
k get secret -o yaml
echo 'c3lHNHZabk1pRVhSMnZFOEhIRUhGV0VGeg==' | base64 -d
gc --amend --no-edit
gc --amend --no-edit
while [ $(fluxctl list-images -w monitoring:helmrelease/***REMOVED***-prometheus-exporter| grep 0.3.7| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 10; done;
export FLUX_FORWARD_NAMESPACE=kuberhealthy
export FLUX_FORWARD_NAMESPACE=kube-system
gc --amend --no-edit
fluxctl sync
while [ $(fluxctl list-images -w monitoring:helmrelease/***REMOVED***-prometheus-exporter| grep 0.3.9| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
k apply -f releases/monitoring/scrapers.yaml
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y   | helm template   ../***REMOVED***-synthetic-transactions/deploy/***REMOVED***-synthetic-tx -x templates/validate-order.yaml
source ~/workspace/.env/bin/activate
fluxctl sync
fluxctl sync
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
k exec -it ubuntu-b7d6cb9c6-x28k7 zsh
k edit deploy ***REMOVED***-***REMOVED***-pr-1011
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
gc --amend --no-edit
gc -m "Use named queries"
gc -m "Use named queries"
gc --amend --no-edit
gc --amend --no-edit
tmux attach -d -t 0
fluxctl sync
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm template   ./deploy/***REMOVED***-prometheus-exporter -x templates/deployment.yaml
source ~/workspace/.env/bin/activate
k apply -f releases/monitoring/scrapers.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm template   ./deploy/***REMOVED***-prometheus-exporter -x templates/deployment.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm template   ./deploy/***REMOVED***-prometheus-exporter -x templates/deployment.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm template   ./deploy/***REMOVED***-prometheus-exporter -x templates/deployment.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm upgrade -i ***REMOVED***-prometeheus-exporter ./deploy/***REMOVED***-prometheus-exporter
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
gc --amend --no-edit
gc --amend --no-edit
fluxctl sync
./prometheus-exporter --root-password=testing123 -e http://localhost:2379
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
fluxctl sync
k apply -f releases/monitoring/scrapers.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm upgrade -i ***REMOVED***-prometeheus-exporter ./deploy/***REMOVED***-prometheus-exporter
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kcn monitoring
fluxctl sync
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
k apply -f monitoring/grafana-dashboards.yaml
kubectl get helmrelease/***REMOVED***-prometheus-exporter -o yaml | yq .spec.values -y   | helm upgrade -i ***REMOVED***-prometeheus-exporter ../***REMOVED***-prometheus-exporter/deploy/***REMOVED***-prometheus-exporter
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
k apply -f monitoring/grafana-dashboards.yaml
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-999vd
assume-role.sh 970357
fluxctl list-images
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-999vd
k delete job descheduler-manual-001 descheduler-manual-002 descheduler-manual-003 
kubectl create job --from=cronjob/descheduler descheduler-manual-003
code ~/.oh-my-zsh/custom/profile.zsh
zshreload
tar -zcvf ScaledDi.tar.gz *
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
GOMODULE142=on go get github.com/golangci/golangci-lint/cmd/golangci-lint@v1.27.0
git rebase origin/master
gc --amend --no-edit
gc -m "Implemented gauge."
gc -m "Fixed unit terst"
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
startxfce4
wire diff ./wire
gc --amend -m "Testing ingress"
git rebase -i HEAD~3
git rebase -i HEAD~7
g cherry-pick --continue
g cherry-pick --continue
gc -m "Inject NamingHelper as a dependency."
gc --amend --no-edit
gc --amend --no-edit
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
gc --amend --no-edit
gc --amend --no-edit
kl -f -n kube-system vpa-updater-76f446d97c-999vd
k apply -f ./releases/monitoring/grafana-dashboards.yaml
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
assume-role.sh 544931
kl -f -n kube-system vpa-updater-76f446d97c-999vd
kl -f -n kube-system vpa-updater-76f446d97c-999vd
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
***REMOVED***-cli startPipeline  -b master   -r podinfo -c ddd40ed
k get cm -o yaml ***REMOVED***
idea >/dev/null 2>&1 &
***REMOVED***-cli startPipeline  -b master   -r podinfo -c aea3821
***REMOVED***-cli startPipeline  -b master   -r podinfo -c aea3821
***REMOVED***-cli startPipeline  -b master   -r podinfo -c aea3821
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
fluxctl sync
fluxctl sync
fluxctl sync
kcn kuberhealthy
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
gc --amend --no-edit
fluxctl list-images
while [ $(fluxctl list-images -w monitoring:helmrelease/***REMOVED***-prometheus-exporter| grep 0.3.9| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
fluxctl sync
fluxctl sync
kl -f alertmanager-prom-operator-prometheus-o-alertmanager-0 alertmanager
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
fluxctl sync
fluxctl sync
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.7.15| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx-ginkgo | grep 2.0.9| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
fluxctl list-images
kdelp -n monitoring alertmanager-prom-operator-prometheus-o-alertmanager-0
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm2 upgrade -i prom-operator stable/prometheus-operator -f - 
source ~/workspace/.env/bin/activate
k exec -it ubuntu-b7d6cb9c6-x28k7 zsh
k exec -it -n podinfo ubuntu-b7d6cb9c6-8xgmx zsh
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
kubectl create job --from=cronjob/descheduler descheduler-manual-003
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
kubectl create job --from=cronjob/descheduler descheduler-manual-003 -n kube-system
helm3 upgrade -i helm-operator-etcd fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="etcd" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s \\
--set image.tag=1.1.0
helm3 upgrade -i helm-operator-monitoring fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="monitoring" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s  \\
--set image.tag=1.1.0
helm3 upgrade -i helm-operator-monitoring fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="monitoring" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s  \\
--set image.tag=1.1.0
helm3 upgrade -i helm-operator-monitoring fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="monitoring" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=1Gi \\
--set resources.requests.cpu=1000m \\
--set configureRepositories.enable=true \\
--set nodeSelector."dedicated"=helmOperator \\
--set tolerations\[0\].effect=NoSchedule \\
--set tolerations\[0\].key=dedicated \\
--set tolerations\[0\].operator=Equal \\
--set tolerations\[0\].value=helmOperator  \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s
kubectl create job --from=cronjob/descheduler descheduler-manual-004 -n kube-system
helm3 upgrade -i helm-operator-external-secrets fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="external-secrets" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=256Mi \\
--set resources.requests.cpu=256m \\
--set resources.limits.memory=512Mi \\
--set resources.limits.cpu=512m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s  \\
--set image.tag=1.1.0
fluxctl sync
fluxctl sync
kubectl create job --from=cronjob/descheduler descheduler-manual-005 -n kube-system
kubectl create job --from=cronjob/descheduler descheduler-manual-006 -n kube-system
kl -f -n kube-system descheduler-manual-005-s2p6m
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl sync
fluxctl sync
helm3 upgrade -i helm-operator-kube-system fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="kube-system" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=256Mi \\
--set resources.requests.cpu=256m \\
--set resources.limits.memory=512Mi \\
--set resources.limits.cpu=512m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s \\
--set image.tag=1.1.0
fluxctl sync
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
k apply -f ./releases/monitoring/grafana-dashboards.yaml
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
k apply -f ./releases/monitoring/
kl -f -n kube-system vpa-updater-76f446d97c-999vd
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
k apply -f ./releases/monitoring/prometheus-operator.yaml
kl -f -n kube-system vpa-updater-76f446d97c-999vd
kl -f -n kube-system descheduler-manual-006-7v9rr
k apply -f ./releases/monitoring/
fluxctl sync
k apply -f ./releases/monitoring/
kl -f -n kube-system vpa-updater-76f446d97c-999vd
k apply -f ./releases/monitoring/
kl -f -n kube-system vpa-updater-76f446d97c-999vd
fluxctl sync
k apply -f ./releases/monitoring/
assume-role.sh 418355
k apply -f ./releases/monitoring/thanos.yaml
k apply -f ./releases/monitoring/thanos.yaml
k exec -it -n podinfo ubuntu-b7d6cb9c6-8xgmx zsh
k apply -f ./releases/monitoring/prometheus-operator.yaml
k drain ip-10-214-10-241.ec2.internal --delete-local-data --ignore-daemonsets
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
k apply -f ./releases/kuberhealthy/
kubectl create job --from=cronjob/descheduler descheduler-manual-006 -n kube-system
fluxctl sync
k drain ip-10-214-9-218.ec2.internal --ignore-daemonsets --delete-local-data --force
k apply -f ./releases/monitoring/prometheus-operator.yaml
k apply -f ./releases/monitoring/prometheus-operator.yaml
git reset --hard origin/master
git reset --hard origin/master
fluxctl sync
kdelp -n kube-system flux-system-655df47c4c-hn2lx
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
kcn monitoring
k apply -f releases/kube-system/helm-operator-vpa.yaml
kl -f -n kube-system flux-system-655df47c4c-vtqgh
tkn pr list -n synthetictx  | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
idea >/dev/null 2>&1 &
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
helm3 upgrade -i helm-operator-external-secrets fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="external-secrets" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=128Mi \\
--set resources.requests.cpu=128m \\
--set resources.limits.memory=512Mi \\
--set resources.limits.cpu=512m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s  \\
--set image.tag=1.1.0
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
k apply -f ./releases/monitoring/prometheus-operator.yaml
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
./synthetic-transaction import
export DEVHUB_URL=https://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
./synthetic-transaction import
./synthetic-transaction import
export GITHUB_TOKEN=***REMOVED***
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
cd ***REMOVED***-synthetic-transactions
./synthetic-transaction import
fluxctl sync
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
./synthetic-transaction import
gc --amend --no-edit
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r podinfo -c aea3821
kcn ***REMOVED***
***REMOVED***-cli startPipeline  -b master   -r podinfo -c aea3821
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
fluxctl sync
kl -f flux-system-54f8b6c77c-5hc7h
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
fluxctl sync
k get khcheck -n kuberhealthy
k apply -f ./releases/kuberhealthy/
k get khcheck -n kuberhealthy
helm3 upgrade -i helm-operator-podinfo-staging fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="podinfo-staging" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=512Mi \\
--set resources.requests.cpu=128m \\
--set resources.limits.memory=1Gi \\
--set resources.limits.cpu=512m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s  \\
--set image.tag=1.1.0
fluxctl sync
k apply -f ./releases/monitoring/elasticsearch-vpa.yaml
fluxctl sync
k apply -f monitoring/kibana-elasticsearch.yaml
k apply -f ./releases/monitoring/kibana-elasticsearch.yaml
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
export DEVHUB_URL=https://***REMOVED***-impdh-pr-977.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
vi ~/.***REMOVED***-cli.yaml
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
kcn impdh-***REMOVED***-pr-980
export DEVHUB_URL=https://***REMOVED***-impdh-pr-980.***REMOVED***.k8.***REMOVED***.com
k api-resources | grep import
k get importorder 
./synthetic-transaction import
gco v2020.015.14
k apply -f ./releases/monitoring/grafana-dashboards.yaml
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
./synthetic-transaction import
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r impdh -c 351931f -p 1017
vi ~/.***REMOVED***-cli.yaml
export DEVHUB_URL=https://***REMOVED***-impdh-pr-980.***REMOVED***.k8.***REMOVED***.com
./synthetic-transaction import
kcn impdh-***REMOVED***-pr-980
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED*** -c 6ec1d06
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED*** -c 6ec1d06
vi ~/.***REMOVED***-cli.yaml
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED*** -c 6ec1d06
gco -b hotfix-pipelineInitCommand
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED*** -c 6ec1d06
fluxctl sync
k apply -f ./releases/staging/***REMOVED***.yaml
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl sync
k describe statefulset elasticsearch-master
k describe daemonset -n monitoring prom-operator-prometheus-node-exporter
assume-role.sh 508193
source ~/workspace/.env/bin/activate
kubectl create job --from=cronjob/descheduler descheduler-manual-001 -n kube-system
kgp -n kube-system
kl -f -n kube-system descheduler-manual-001-5psbg
kubectl create job --from=cronjob/descheduler descheduler-manual-001 -n kube-system
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
kcn impdh-staging
./synthetic-transaction import
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
./synthetic-transaction import -g  ***REMOVED***
export GITHUB_TOKEN=***REMOVED***
fluxctl completion zsh > ~/.oh-my-zsh/completions/_fluxctl
zshreload
git clone git@github.com:***REMOVED***/calculator.git
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
./synthetic-transaction import --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000 --target-repo ***REMOVED***/***REMOVED***-calculator-staging
zshreload
make build
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.7.15| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
fluxctl list-images
export DEVHUB_URL=https://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
export GITHUB_TOKEN=***REMOVED***
./synthetic-transaction import --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.8.0| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
fluxctl sync
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
k apply -f ./releases/kuberhealthy/
tmux
zshreload
idea >/dev/null 2>&1 &
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k apply -f ./releases/kuberhealthy/
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.8.2| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.8.3| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done;
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
kl -f -n kube-system vpa-updater-76f446d97c-999vd updater 
k describe daemonset -n monitoring prom-operator-prometheus-node-exporter 
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
source ~/workspace/.env/bin/activate
k apply -f releases/monitoring/prometheus-operator.yaml
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
k apply -f releases/monitoring/prometheus-operator.yaml
k delete hr prom-operator
k apply -f releases/monitoring/prometheus-operator.yaml
k describe daemonset -n monitoring prom-operator-prometheus-node-exporter 
fluxctl sync
kdhr prom-operator
k describe daemonset -n monitoring prom-operator-prometheus-node-exporter 
k rollout restart daemonset prom-operator-prometheus-node-exporter
k apply -f releases/monitoring/prometheus-operator.yaml
k rollout restart daemonset prom-operator-prometheus-node-exporter
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kl -f -n kube-system descheduler-manual-002-tq7jq descheduler 
kubectl create job --from=cronjob/descheduler descheduler-manual-002 -n kube-system
kl -f -n kube-system descheduler-manual-001-5psbg
fluxctl sync
kdp prom-operator-prometheus-node-exporter-q6kgv
k rollout restart daemonset prom-operator-prometheus-node-exporter
k rollout restart daemonset prom-operator-prometheus-node-exporter
k drain  ip-10-214-9-197.ec2.internal --ignore-daemonsets --delete-local-data --force
kl -f -n kube-system descheduler-manual-001-w2khw
k drain  ip-10-214-9-197.ec2.internal --ignore-daemonsets --delete-local-data --force
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-9-63.ec2.internal ip-10-214-6-154.ec2.internal ip-10-214-11-126.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-9-63.ec2.internal ip-10-214-6-154.ec2.internal ip-10-214-11-126.ec2.internal
kdno ip-10-214-10-78.ec2.internal
k drain ip-10-214-7-55.ec2.internal ip-10-214-8-218.ec2.internal ip-10-214-9-111.ec2.internal --ignore-daemonsets --delete-local-data
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-9-63.ec2.internal ip-10-214-6-154.ec2.internal ip-10-214-11-126.ec2.internal
kdelp -n codefix-staging aurea-codefix-ui-application-staging-0
k rollout restart daemonset prom-operator-prometheus-node-exporter
k rollout restart daemonset prom-operator-prometheus-node-exporter
kubectl create job --from=cronjob/descheduler descheduler-manual-001 -n kube-system
kl -f -n kube-system descheduler-manual-001-w2khw
kubectl create job --from=cronjob/descheduler descheduler-manual-002 -n kube-system
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-9-63.ec2.internal ip-10-214-6-154.ec2.internal ip-10-214-11-126.ec2.internal
source ~/workspace/.env/bin/activate
kubectl get helmrelease/prom-operator -o yaml | yq .spec.values -y  | helm upgrade -i prom-operator stable/prometheus-operator -f - 
kubectl get helmrelease/ingress-internal -o yaml | yq .spec.values -y  | helm upgrade -i ingress-internal stable/nginx-ingress:1.24.5 -f - 
k apply -f releases/monitoring/prometheus-operator.yaml
assume-role.sh 224491
sudo ./hack/vpa-down.sh
k apply -f releases/kube-system/nginx-ingress.yaml
kubectl get helmrelease/ingress-internal -o yaml | yq .spec.values -y  | helm upgrade -i ingress-internal stable/nginx-ingress --version 1.24.6 -f - 
helmfile -f helmfiles/100-unleash.yaml -e dev destoy
helmfile -f helmfiles/080-jaeger-tracing.yaml -e dev apply
k exec -it -n podinfo ubuntu-b7d6cb9c6-8xgmx zsh
helmfile -f helmfiles/000-nginx-ingress.yaml -e prod apply
helmfile -f helmfiles/000-nginx-ingress.yaml -e prod apply
k exec -it -n podinfo ubuntu-b7d6cb9c6-8n5t9 bash
helmfile -f helmfiles/000-nginx-ingress.yaml -e prod apply
k exec -it -n podinfo ubuntu-b7d6cb9c6-8n5t9 zsh
k scale deployment --replicas 0 helm-operator-kube-system
fluxctl sync
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
fluxctl list-images
sudo ./hack/vpa-up.sh
./hack/vpa-down.sh
kdelp metrics-server-547956ddfc-htmxv
k apply -f releases/monitoring/prometheus-operator.yaml
helm3 upgrade -i flux-podinfo- fluxcd/flux \\
--namespace podinfo \\
--set git.url=git@github.com:***REMOVED***/podinfo-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-podinfo- \\
--set registry.includeImage="***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/podinfo*" \\
--set registry.automationInterval="5m" \\
--set git.pollInterval=1m  \\
--set resources.requests.cpu=128m \\
--set resources.requests.memory=512Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
fluxctl sync
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED***-staging -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED***-staging -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
source ~/workspace/.env/bin/activate
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
assume-role.sh 337485 
assume-role.sh 224491
git clone git@github.com:***REMOVED***/***REMOVED***-calculator-staging.git calculator-staging
kcn ***REMOVED***
k edit deploy ***REMOVED***
k apply -f servicemon.yaml
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
kubectl get helmrelease/***REMOVED***-staging -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
helm list --namespace ***REMOVED***
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
k apply -f releases/***REMOVED***
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
k apply -f releases/***REMOVED***
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
kl -f ***REMOVED***-***REMOVED***-85dcb489b7-ff5pq
kl -f ***REMOVED***-***REMOVED***-7f5df8867c-n9c4g
k apply -f releases/***REMOVED***
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
k apply -f releases/***REMOVED***
fluxctl sync
helm list --max 1000 | grep ***REMOVED***
***REMOVED***-cli startPipeline  -b master   -r ***REMOVED*** -c 6ec1d06
k apply -f releases/***REMOVED***
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
k rollout restart deployment
k rollout restart deployment ***REMOVED***-***REMOVED*** charts
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
tkn pr list
helm list --max 1000 | grep ***REMOVED***
helm delete --purge ***REMOVED***-***REMOVED***-pr-1009 ***REMOVED***-***REMOVED***-pr-1017 ***REMOVED***-***REMOVED***-pr-1020 ***REMOVED***-***REMOVED***-pr-1024 ***REMOVED***-***REMOVED***-pr-1026 ***REMOVED***-***REMOVED***-pr-1027 ***REMOVED***-***REMOVED***-pr-221 ***REMOVED***-***REMOVED***-pr-1229 ***REMOVED***-***REMOVED***-pr-239
k get serviceaccount -o yaml
helm list --max 1000 | grep ***REMOVED***
helm delete --purge impdh-***REMOVED***-faustoc  impdh-***REMOVED***-shah impdh-***REMOVED***-stefan
fluxctl sync
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
assume-role.sh 955032
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
docker build -f ./builder-go/Dockerfile-trusty-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest builder-go
docker build -f ./builder-go/Dockerfile-trusty-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest builder-go
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
git rebase origin/master --autostash
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:javier 
docker build -f ./builder-go/Dockerfile-xenial-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest builder-go
kubectl convert -f ./***REMOVED***.yaml --output-version apps/v1
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest
kubectl convert --output-version apps/v1 -f -
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert --output-version apps/v1 -f -
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
kgd -o yaml ***REMOVED***-dev
kgd -o yaml ***REMOVED***-dev
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
k apply -f releases/staging
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-dev ./charts/***REMOVED*** -f - 
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert --output-version apps/v1 -f -
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl apply -f -
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
k apply -f releases/***REMOVED***-pr-980/
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert --output-version apps/v1 -f -
k apply -f releases/***REMOVED***-pr-980/
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl apply -f -
kgd -o yaml 
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert --output-version extensions/v1beta1 -f -
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert --output-version apps/v1 -f -
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
k convert --help
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert -f -  
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i impdh-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
k delete deploy release-name
helm delete --purge impdh-***REMOVED***-pr-980
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
kgd
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template -n ***REMOVED***-pr-980  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert -f - | k apply -f  -
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template -n ***REMOVED***-***REMOVED***-pr-980  ./charts/***REMOVED*** -x templates/deployment.yaml -f - | kubectl convert -f - | k apply -f  -
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
k apply -f releases/***REMOVED***-pr-980/
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
kgd -o yaml ***REMOVED***-***REMOVED***-pr-980
k get deployments.v1.apps
k apply -f releases/***REMOVED***-pr-980/
helm delete ***REMOVED***-***REMOVED***-pr-980 --purge
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
git rebase --skip
k apply -f releases/***REMOVED***-pr-980/
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED*** ./charts/***REMOVED*** -f - 
tkn pr list -n synthetictxstaging | grep -i runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
docker build -f ./builder-go/Dockerfile-xenial-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest builder-go
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
docker build -f ./builder-go/Dockerfile-trusty-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest builder-go
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest
helm push ./***REMOVED*** chartmuseum -p "***REMOVED***"  -u chart-user
kubectl config use-context dev-cluster
helm push ./***REMOVED*** chartmuseum
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest
docker build -f ./builder-go/Dockerfile-xenial-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest builder-go
docker build -f ./builder-go/Dockerfile-trusty-go -t ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest builder-go
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:latest
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-trusty-go:javier
docker push ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com/***REMOVED***/builder-xenial-go:latest
kubectl config use-context dev-cluster
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kl -f -n kube-system flux-system-7ff956fcdd-lpp7n
k apply -f releases/javier/
fluxctl sync
fluxctl sync
fluxctl sync
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
git clone git@github.com:***REMOVED***/knova-gitops.git
git clone https://github.com/***REMOVED***/cloudfix-gitops.git
git clone git@github.com:***REMOVED***/cloudfix-gitops.git
git clone https://github.com/***REMOVED***/cloudfix-gitops.git
kubectl config use-context STAGING
kcn cloudfix-staging
k exec -it -n podinfo ubuntu-b7d6cb9c6-8n5t9 zsh
k apply -f releases/denis/
k apply -f releases/staging/
k apply -f releases/staging/
k apply -f releases/staging/
k apply -f releases/dev/
k apply -f releases/staging/
deactivate
k delete hr cloudfix-platform-invocables
helm3 delete helm-operator-cloudfix-staging
helm3 delete helm-operator-cloudfix-staging -n kube-system
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
git clone https://***REMOVED***@github.com/***REMOVED***/cloudfix-gitops.git
git clone https://***REMOVED***@github.com/***REMOVED***/KNOV-gitops.git
git clone https://***REMOVED***@github.com/***REMOVED***/mykerio-hub.git
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
git clone https://***REMOVED***@github.com/***REMOVED***/MyKerio-gitops.git
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
k apply -f releases/prod/
git clone https://***REMOVED***@github.com/***REMOVED***/TBGTM-gitops.git
git clone git@github.com:***REMOVED***/***REMOVED***-infra-as-code.git
git clone https://***REMOVED***@github.com/***REMOVED***/atsea-ng-gitops
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
kgns --all-namespaces | grep staging
git clone git@github.com:***REMOVED***/***REMOVED***-infra-as-code.git
git clone https://***REMOVED***@github.com/***REMOVED***/devflows-gitops
kgns --all-namespaces | grep staging
khr --all-namespaces | grep -i TBGTM
k apply -f releases/prod/
k apply -f releases/prod/
k delete hr webmethods-server-production 
k apply -f releases/prod/
k apply -f releases/prod/
k apply -f namespaces/prod.yaml
kgns --all-namespaces | grep staging
git clone https://***REMOVED***@github.com/***REMOVED***/aws-utils-gitops
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
k apply -f releases/dev/
k apply -f releases/staging/
kcn  sliao-staging
k apply -f releases/prod/
helm delete --purge  operations-svn-prod
k apply -f releases/prod/
kcn  sliao-prod
helm list --namespace sliao-operations-svn-pr-4
kgns --all-namespaces | grep staging
git clone https://***REMOVED***@github.com/***REMOVED***/synthetictxstaging-gitops
git clone https://***REMOVED***@github.com/***REMOVED***/standing-meetings-gitops
git clone https://***REMOVED***@github.com/***REMOVED***/sococo-gitops
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
kgns --all-namespaces | grep staging
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
k get deployments.v1.apps
fluxctl sync
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
assume-role.sh 955032
kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.5.3/tekton-dashboard-release-readonly.yaml
kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.6.0/tekton-dashboard-release-readonly.yaml
kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.5.3/tekton-dashboard-release-readonly.yaml
kl -f tekton-dashboard-79d9f8bc4b-gct8r
kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.4.1/release.yaml
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
***REMOVED***-cli startPipeline  -b master   -r impdh-gitops -c 0272d0e
kcn synthetictx
tkn pr list
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
k scale deployment --replicas 0 helm-operator-monitoring helm-operator-podinfo-staging  helm-operator-external-secrets helm-operator-etcd helm-operator-sonar  helm-operator-kuberhealthy helm-operator-kube-system
fluxctl sync
k scale deployment --replicas 0 helm-operator-monitoring helm-operator-podinfo-staging  helm-operator-external-secrets helm-operator-etcd helm-operator-sonar  helm-operator-kuberhealthy helm-operator-kube-system
k scale deployment --replicas 0 helm-operator-monitoring helm-operator-podinfo-staging  helm-operator-external-secrets helm-operator-etcd helm-operator-sonar  helm-operator-kuberhealthy helm-operator-kube-system
k delete externalsecret ***REMOVED***
k apply -f releases/***REMOVED***
k scale deployment --replicas 1 helm-operator-monitoring helm-operator-podinfo-staging  helm-operator-external-secrets helm-operator-etcd helm-operator-sonar  helm-operator-kuberhealthy helm-operator-kube-system
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
source ~/workspace/.env/bin/activate
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
fluxctl sync
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
kl -f prom-operator-grafana-5dc9f49766-5snrs grafana-sc-dashboard
k apply -f releases/staging/
kubectl get helmrelease/***REMOVED***-staging -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
k apply -f releases/staging/
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.12/bin/linux/amd64/kubectl
k apply -f releases/staging/
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** -f - 
k apply -f releases/staging
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm template -n ***REMOVED***-staging ./charts/***REMOVED*** 
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED*** 
k apply -f releases/staging
kcn impdh-staging
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-10-6.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-10-6.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-6-130.ec2.internal
kcn impdh-***REMOVED***-pr-1020
k rollout restart deployment prom-operator-grafana
k rollout restart deployment ***REMOVED***-staging
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-11-193.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-10-118.ec2.internal  ip-10-214-6-132.ec2.internal 
k drain   --ignore-daemonsets --delete-local-data --force   ip-10-214-10-48.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-10-92.ec2.internal
k drain   --ignore-daemonsets --delete-local-data --force  ip-10-214-6-85.ec2.internal
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
k apply -f releases/staging
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED***  -f -
source ~/workspace/.env/bin/activate
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
eksctl completion zsh > ~/.oh-my-zsh/completions/_eksctl
zshreload
kubectl create job --from=cronjob/descheduler descheduler-manual-001 -n kube-system
kl -f vpa-updater-6754d65d8d-dbfgx
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
zshreload
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
zshreload
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
***REMOVED***-cli createWorkspace -p synthetictxstaging -t import-automation-tw -u ***REMOVED***
***REMOVED***-cli createWorkspace -p synthetictxstaging -t import-automation-tw -u ***REMOVED***
***REMOVED***-cli createWorkspace -p synthetictxstaging -t import-automation-tw -u ***REMOVED***
k apply -f releases/staging
kubectl get helmrelease/***REMOVED*** -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-staging ./charts/***REMOVED***  -f -
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-980 ./charts/***REMOVED*** -f - 
source ~/workspace/.env/bin/activate
k apply -f releases/staging/
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-1020 ./charts/***REMOVED*** -f - 
k apply -f releases/***REMOVED***-pr-1020
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-1020 ./charts/***REMOVED*** -f - 
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-***REMOVED***-pr-1020 ./charts/***REMOVED*** -f - 
k apply -f releases/staging
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
k delete externalsecret ***REMOVED***-e2e-tests
k apply -f releases/kuberhealthy/
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
k apply -f releases/kuberhealthy/
k apply -f releases/kuberhealthy/
kubectl get helmrelease/kuberhealthy -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
assume-role.sh 056954
k delete externalsecret kuberhealthy
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com
***REMOVED***-cli createWorkspace --productName javier-lol -t import-automation-tw -u ***REMOVED***
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
helm list --max 10000 --all  | grep vm | grep workspace
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
helm3 2to3 convert filebeat
helm3 2to3 convert logstash
docker run -it -p 2379:2379 --rm -e ETCD_ROOT_PASSWORD=testing123 --name etcd bitnami/etcd
docker build -f ***REMOVED***/Dockerfile .
docker run -it  --rm --name helm3 b2a535027ff7
docker build -f ***REMOVED***/Dockerfile .
docker run -it  --rm --name helm3 740bd27ff965
helm3 2to3 convert thanos
helm3 2to3 convert thanos --delete-v2-releases
helm list --namespace monitoring
k apply -f releases/monitoring/
k apply -f releases/monitoring/
helm3 2to3 convert thanos --delete-v2-releases
helm list --namespace monitoring
k apply -f releases/monitoring/
kubectl config use-context dev-cluster
kubectl config use-context STAGING
helm list | awk '{print $1}' | xargs -I{} helm3 2to3 convert {}
helm3 list
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
helm list --max 10000 --all  | grep vm | grep workspace
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
helm delete vmctl-synthetic-workspace
kdelns synthetic-workspace  synthetic-workspace-dev  synthetic-workspace-staging synthetic-workspace-prod
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com
helm plugin install https://github.com/chartmuseum/helm-push.git
helm init --client-only
helmfile -f helmfiles-ia/000-ia-***REMOVED***.yaml -e dev apply
helm repo add chartmuseum https://charts.***REMOVED***.k8.***REMOVED***.com --password "***REMOVED***" --username chart-user
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
helm3 upgrade -i helm-operator-kube-system fluxcd/helm-operator \\
--namespace kube-system \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="kube-system" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="20" \\
--set logFormat="json" \\
--set resources.requests.memory=512Mi \\
--set resources.requests.cpu=128m \\
--set resources.limits.memory=1Gi \\
--set resources.limits.cpu=512m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s \\
--set image.tag=1.1.0
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED***
fluxctl sync
fluxctl sync
fluxctl sync
k apply -f releases/monitoring/
k apply -f releases/monitoring/
source ~/workspace/.env/bin/activate
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl list-images
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace kube-system \\
--set git.url=git@github.com:***REMOVED***/***REMOVED***-application-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="gcr.io/tekton-releases/*\,docker.io/bitnami/postgresql:*" \\
--set registry.automationInterval="5m" \\
--set git.pollInterval=1m  \\
--set resources.requests.cpu=128m \\
--set resources.requests.memory=512Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
fluxctl sync
fluxctl sync
fluxctl list-images
./hack/vpa-down.sh
fluxctl sync
fluxctl list-images
fluxctl list-workloads
k apply -f releases/kube-system/
fluxctl sync
./hack/vpa-down.sh
fluxctl sync
fluxctl list-workloads
k apply -f releases/kube-system/
fluxctl list-images
fluxctl sync
fluxctl sync
k exec -it -n podinfo ubuntu-b7d6cb9c6-wfgg4  zsh
fluxctl sync
k apply -f releases/kube-system/
k apply -f releases/kube-system/
k apply -f releases/kube-system/
helm list --max 10000 --all  | grep vm | grep workspace
./hack/vpa-up.sh
fluxctl sync
k scale deployment --replicas  1 helm-operator-monitoring
k apply -f releases/kube-system/
k scale deployment --replicas  0 helm-operator-monitoring
k apply -f releases/kube-system/
k apply -f releases/kube-system/
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml
k apply -f releases/kube-system/
k apply -f releases/kube-system/
fluxctl sync
k apply -f releases/kube-system/
k scale deployment --replicas  1 helm-operator-monitoring
k apply -f releases/kube-system/
kubectl get helmrelease/kubernetes-dashboard -o yaml | yq .spec.values -y  | helm upgrade -i kubernetes-dashboard stable/kubernetes-dashboard -f -
k apply -f releases/kube-system/
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k apply -f releases/kube-system/
kubectl get helmrelease/kubernetes-dashboard -o yaml | yq .spec.values -y  | helm upgrade -i kubernetes-dashboard stable/kubernetes-dashboard -f -
k rollout restart deployment ***REMOVED***-***REMOVED***
fluxctl sync
fluxctl sync
fluxctl sync
assume-role.sh 132074
k apply -f releases/kube-system/
k apply -f releases/kube-system/kubernetes-dashboard-csrf.yaml
k scale deployment --replicas  0 helm-operator-monitoring
k apply -f releases/kube-system/kubernetes-dashboard-csrf.yaml
idea >/dev/null 2>&1 &
k apply -f releases/kube-system/
k apply -f releases/kube-system/
k scale deployment --replicas  1 helm-operator-monitoring
tkn pr list -n synthetictxstaging | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictxstaging pr delete --force
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
k apply -f releases/kube-system/
k apply -f releases/kube-system/
kubectl get helmrelease/grafana-dashboards -o yaml | yq .spec.values -y  | helm upgrade -i grafana-dashboards ./charts/***REMOVED***-grafana-dashboards -f -
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator import
tkn pr list -n synthetictx | grep -vi runn | awk '{printf "%s\n",$1}' | xargs tkn -n synthetictx pr delete --force
fluxctl sync
fluxctl sync
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED***
./synthetic-transaction completion zsh > ~/.oh-my-zsh/completions/_synthetic-transaction
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED***
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED***
helm list --max 10000 --all  | grep vm | grep workspace
***REMOVED***-cli createWorkspace --productName synthetic-workspace -t import-automation-tw -u ***REMOVED***
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED*** --synthetic-workspace synthethix-workspace
helm list --max 100000 --all  | grep dashboard
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED*** --synthetic-workspace synthetic-workspace-staging
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl list-images
fluxctl sync
make build && ./synthetic-transaction create-workspace --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --github-user ***REMOVED*** --synthetic-workspace synthetic-workspace-stg
GITHUB_USER=***REMOVED*** make build && ./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --synthetic-workspace synthetic-workspace-stg create-workspace
make build && ./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-staging.***REMOVED***.k8.***REMOVED***.com --synthetic-workspace synthetic-workspace-stg create-workspace
fluxctl sync
fluxctl sync
docker run -it  --rm --name helm3 04dfdc0ac9b0
docker run -it  --rm --name helm3 447d6c5e0ca6
while [ $(fluxctl list-images -w kuberhealthy:helmrelease/***REMOVED***-synthetic-tx | grep 0.8.6| wc -l) -eq 0 ]; do echo 'Waiting...'; sleep 5; done; echo 'Finished!'
fluxctl list-images
fluxctl list-workloads
fluxctl sync
helm list --max 100000 --all  | grep workspace
assume-role.sh 656617
assume-role.sh 567261
k scale deployment --replicas  1 helm-operator-monitoring
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator-staging import
helm list --max 10000 --all | grep work
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator import
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator import
***REMOVED***-cli createWorkspace -p synthetictx -t import-automation-tw -u ***REMOVED*** -r
***REMOVED***-cli submit  -x -f ./importOrder.yaml -g 4a5b8dc86d3c469cb25babcb0750b5c8ff24d000
./synthetic-transaction  --***REMOVED***-url http://***REMOVED***-impdh-***REMOVED***.***REMOVED***.k8.***REMOVED***.com --github-token ***REMOVED*** --target-repo ***REMOVED***/***REMOVED***-calculator import
k scale --replicas 1 deployment ***REMOVED***-***REMOVED***
fluxctl sync
k apply -f releases/kube-system/
fluxctl sync
git clone git@github.com:***REMOVED***/***REMOVED***-serverless
k scale deployment --replicas 0 helm-operator-kube-system
k scale deployment --replicas 0 flux-system
k scale deployment --replicas 0 helm-operator-kube-system
EKSCTL_EXPERIMENTAL=true eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster eks-flux-stack --region us-east-1
EKSCTL_EXPERIMENTAL=true eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster eks-flux-stack --region us-east-1
EKSCTL_EXPERIMENTAL=true eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-stack --region us-east-1
EKSCTL_EXPERIMENTAL=true eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-stack --region us-east-1
EKSCTL_EXPERIMENTAL=true eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-stack --region us-east-1
kubectl port-forward -n demo svc/podinfo 9898:9898
kubectl port-forward -n demo svc/podinfo 9898:9898
k apply -f base/monitoring/
kubectl config use-context STAGING
kubectl config use-context dev-cluster
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
k apply -f base/monitoring/
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
fluxctl sync
kubectl config use-context STAGING
assume-role.sh 155180
eksctl create cluster --name flux-stack  --region us-east-1 
eksctl create cluster --name flux-stack  --region us-east-1 
eksctl create cluster --name flux-stack  --region us-east-1 
export EKSCTL_EXPERIMENTAL=true
eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-stack --region us-east-1
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
export EKSCTL_EXPERIMENTAL=true
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
k -n podinfo exec -it ubuntu bash
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
fluxctl sync
fluxctl sync
fluxctl sync
eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-helm-op --region us-east-1
kubectl config use-context STAGING
kubectl config use-context ***REMOVED***@flux-stack.us-east-1.eksctl.io
kubectl config use-context ***REMOVED***@flux-stack.us-east-1.eksctl.io
k apply -f flux/
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
kcn feature-toggles
fluxctl sync
fluxctl sync
k apply -f flux/
k get externalsecrets -w
k apply -f releases/feature-toggles/
kcn feature-toggles
k apply -f flux/
k rollout restart deployment helm-operator 
k apply -f flux
k rollout restart deployment helm-operator 
k apply -f flux
k apply -f releases/chart-museum/
k apply -f releases/chart-museum/
k rollout restart deployment helm-operator 
k apply -f flux
k rollout restart deployment helm-operator 
kubectl exec -it -n monitoring ***REMOVED***-prometheus-exporter-bff788d9f-8gb25  bash
k exec -it helm-operator-77f64879bf-x22fd bash
kl -f helm-operator-57f6b866bd-76n9w
k apply -f flux
k exec -it helm-operator-57f6b866bd-76n9w bash
k rollout restart deployment helm-operator 
kubectl config use-context STAGING
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
k rollout restart deployment helm-operator 
k rollout restart deployment chart-museum-chartmuseum
kcn chart-museum
kcn flux
k rollout restart deployment chart-museum-chartmuseum
k rollout restart deployment helm-operator 
k exec -it helm-operator-6b4f9b5bbb-wx25s bash
k apply -f flux/
k exec -it helm-operator-78fb85d8f8-h797w bash
fluxctl sync
k rollout restart deployment helm-operator 
k apply -f ./releases/
k exec -it helm-operator-685d56f4d-h6j4t bash
kl -f helm-operator-6cd76df986-2cpt8
fluxctl sync
helm3 repo update
helm3 repo update
make chart
helm repo add chartmuseum https://charts.***REMOVED***.k8.***REMOVED***.com --password "***REMOVED***" --username chart-user
k apply -f releases/feature-toggles/
helm3 repo add eksctlcharts https://charts.***REMOVED***.k8.***REMOVED***.com --password "***REMOVED***" --username chart-user
k apply -f releases/feature-toggles/
k apply -f releases/feature-toggles/
k apply -f releases/feature-toggles/
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
k apply -f releases/chart-museum/
fluxctl sync
fluxctl sync
eksctl update cluster --config-file cluster.yaml --approve
eksctl update cluster --config-file cluster.yaml --approve
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
kubectl config use-context STAGING
assume-role.sh 343078
./hack/vpa-down.sh
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
kubectl config use-context STAGING
./hack/vpa-up.sh
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
kubectl config use-context ***REMOVED***@flux-stack.us-east-1.eksctl.io
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
fluxctl sync
eksctl create cluster --config-file ./cluster.yaml
eksctl delete cluster --name flux-helm-op
fluxctl sync
fluxctl sync
eksctl delete cluster --name flux-helm-op
eksctl create cluster --config-file ./cluster.yaml
fluxctl sync
export FLUX_FORWARD_NAMESPACE=kube-system
fluxctl sync
kcn ***REMOVED***
kcn kuberhealthy
eksctl create cluster --config-file ./cluster.yaml
fluxctl sync
eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-helm-op --region us-east-1 --color true
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-stack --region us-east-1
fluxctl sync
fluxctl sync
fluxctl sync
vi ~/.tmux.conf
set -g mouse on
set -g mouse on
tmux
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
kdhr reloader
zshreload
zshreload
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-helm-op --region us-east-1 --color true
helm3 completion zsh > ~/.oh-my-zsh/completions/_helm3
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never 
kcn chart-museum
kcn flux
k apply -f flux/
k apply -f flux/
kcn chart-museum
k apply -f releases/kuberhealthy/
eksctl completion zsh > ~/.oh-my-zsh/completions/_eksctl
zshreload
assume-role.sh 306240
kubectl config use-context STAGING
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
kubectl get cm git-ssh-config --namespace=kube-system --export -o yaml |   kubectl apply --namespace=flux -f -
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
zshreload
zshreload
fluxctl sync
helm3 upgrade -i helm-operator-kube-system fluxcd/helm-operator \\
--namespace flux \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="kube-system" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="40" \\
--set logFormat="json" \\
--set resources.requests.memory=512Mi \\
--set resources.requests.cpu=128m \\
--set resources.limits.memory=2Gi \\
--set resources.limits.cpu=1024m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s \\
--set image.tag=1.1.0
fluxctl sync
fluxctl sync
helm3 upgrade -i flux-system fluxcd/flux \\
--namespace flux \\
--set git.url=git@github.com:***REMOVED***/***REMOVED***-application-gitops \\
--set memcached.enabled=true \\
--set rbac.create=true \\
--set syncGarbageCollection.enabled=true \\
--set serviceAccount.create=true \\
--set serviceAccount.name=flux-system \\
--set registry.excludeImage="k8s.gcr.io/*\,gcr.io/tekton-releases/*\,docker.io/bitnami/postgresql:*" \\
--set registry.automationInterval="5m" \\
--set git.pollInterval=1m  \\
--set resources.requests.cpu=128m \\
--set resources.requests.memory=512Mi \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set extraVolumeMounts\[0\].mountPath=/dockercfg \\
--set extraVolumeMounts\[0\].name=dockerconfig \\
--set extraVolumes\[0\].name=dockerconfig \\
--set extraVolumes\[0\].configMap.name=docker-config-json
helm3 upgrade -i helm-operator-kube-system fluxcd/helm-operator \\
--namespace flux \\
--set git.ssh.configMapName=git-ssh-config \\
--set git.ssh.secretName=git-ssh-secret \\
--set git.pollInterval="10m" \\
--set chartsSyncInterval="10m" \\
--set allowNamespace="kube-system" \\
--set statusUpdateInterval="10s" \\
--set resources.requests.cpu="1" \\
--set workers="40" \\
--set logFormat="json" \\
--set resources.requests.memory=512Mi \\
--set resources.requests.cpu=128m \\
--set resources.limits.memory=2Gi \\
--set resources.limits.cpu=1024m \\
--set configureRepositories.enable=true \\
--set prometheus.enabled=true \\
--set prometheus.serviceMonitor.create=true \\
--set prometheus.serviceMonitor.interval=10s \\
--set image.tag=1.1.0
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
fluxctl sync
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
fluxctl sync
idea >/dev/null 2>&1 &
source ~/workspace/.env/bin/activate
kubectl get helmrelease/***REMOVED***-synthetic-tx -o yaml | yq .spec.values -y  | helm upgrade -i ***REMOVED***-synthetic-tx chartmuseum/***REMOVED***-synthetic-tx -f -
kubectl get helmrelease/***REMOVED***-dev -o yaml | yq .spec.values -y  | helm template  ./charts/***REMOVED*** -x templates/deployment.yaml 
k apply -f chart-museum/
idea >/dev/null 2>&1 &
kcn chart-museum
kl -f alb-ingress-controller-5f54644bcc-nvmpc
fluxctl sync
ohmyzsh
zshreload
zshreload
k apply -f chart-museum/
k apply -f chart-museum/
k apply -f chart-museum/
k apply -f chart-museum/
git clone git@github.com:fluxcd/helm-operator.git
git init
git remote add origin git@github.com:helm/charts.git
git init
git remote add origin git@github.com:helm/charts.git
git remote add origin git@github.com:helm/charts.git
git remote add origin git@github.com:helm/charts.git
kubectl get helmrelease/chart-museum -o yaml | yq .spec.values -y  | helm template  -s templates/ingress.yaml stable/chartmuseum -f -
source ~/workspace/.env/bin/activate
startxfce4
k apply -f chart-museum/
k apply -f chart-museum/
k apply -f chart-museum/
k apply -f chart-museum/
fluxctl sync
eksctl enable profile app-dev  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-helm-op --region us-east-1
ohmyzsh
zshreload
k apply -f chart-museum/
idea .
idea .
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
zshreload
ohmyzsh
zshreload
ohmyzsh
ohmyzsh
zshreload
zshreload
ohmyzsh
zshreload
zshreload
k apply -f ./feature-toggles/
./hack/vpa-up.sh
fluxctl sync
k exec -it ubuntu-54ccfdd5bd-7l82l bash
k apply -f feature-toggles/
helm3 repo add eksctlcharts http://ad9b29b431b694ed7bbbe7d61fb4e2ee-867186941.us-east-1.elb.amazonaws.com --password "***REMOVED***" --username chart-user
helm3 repo remove eksctlcharts
helm3 repo remove eksctlcharts
helm3 repo add eksctlcharts http://2807a9f0-chartmuseum-chart-310c-1472363394.us-east-1.elb.amazonaws.com --password "***REMOVED***" --username chart-user
helm repo add chartmuseum https://charts.***REMOVED***.k8.***REMOVED***.com --password "***REMOVED***" --username chart-user
helm plugin install https://github.com/chartmuseum/helm-push.git
helm push . chartmuseum -p "***REMOVED***"  -u chart-user
zshreload
kl -f alb-ingress-controller-5f54644bcc-nvmpc -n kube-system
gc --amend --no-edit
gc --amend --no-edit
ohmyzsh
zshreload
assume-role.sh 477926
which assumerole.sh
idea assume-role.sh 
assume-role.sh 477926
idea >/dev/null 2>&1 &
idea >/dev/null 2>&1 &
startxfce4
git clone git@github.com:helm/charts.git helm_charts
eksctl create cluster --name eks-flux-stack
eksctl create cluster --config-file ./cluster.yaml
eksctl create cluster --config-file ./cluster.yaml
aws iam create-policy  --policy-name ALBIngressControllerIAMPolicy --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/iam-policy.json
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster flux-helm-op    --override-existing-serviceaccounts \\
    --approve
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster flux-helm-op    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve   --attach-policy-arn arn:aws:iam::***REMOVED***:policy/ALBIngressControllerIAMPolicy
eksctl enable repo  --git-url git@github.com:***REMOVED***/***REMOVED***-eksctl   --git-email ***REMOVED***@***REMOVED***.com  --cluster flux-helm-op --region us-east-1 --color true
export AWS_PROFILE=personal
eksctl enable repo  --config-file ./cluster/cluster.yaml
eksctl create iamserviceaccount  --namespace kube-system  --config-file ./cluster/cluster.yaml    --override-existing-serviceaccounts --approve
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --config-file ./cluster/cluster.yaml 
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster flux-helm-op    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve
idea >/dev/null 2>&1 &
export AWS_PROFILE=personal
fluxctl sync
export AWS_PROFILE=personal
fluxctl sync
ohmyzsh
zshreload
fluxctl sync
helm package .
helm push ./unleash http://2807a9f0-chartmuseum-chart-310c-1472363394.us-east-1.elb.amazonaws.com -p "***REMOVED***"  -u chart-user
aws iam create-policy  --policy-name ALBIngressControllerIAMPolicy --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/iam-policy.json
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster javier    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve   --attach-policy-arn arn:aws:iam::***REMOVED***:policy/ALBIngressControllerIAMPolicy
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster flux-helm-op    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster javier    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve   --attach-policy-arn arn:aws:iam::***REMOVED***:policy/ALBIngressControllerIAMPolicy
eksctl create iamserviceaccount --region us-east-1 --namespace kube-system  --cluster javier    --override-existing-serviceaccounts  --name alb-ingress-controller  --approve   --attach-policy-arn arn:aws:iam::745892955196:policy/ALBIngressControllerIAMPolicy
fluxctl sync
eksctl create iamserviceaccount  --region us-east-1  --name alb-ingress-controller \\
    --namespace kube-system \\
    --cluster javier \\
    --attach-policy-arn arn:aws:iam::745892955196:policy/ALBIngressControllerIAMPolicy \\
    --override-existing-serviceaccounts \\
    --approve
ohmyzsh
zshreload
fluxctl sync
idea >/dev/null 2>&1 &
k apply -f chart-museum/
fluxctl sync
k rollout restart deployment chart-museum-chartmuseum 
fluxctl sync
fluxctl sync
fluxctl list-images
fluxctl list-images
fluxctl list-workloads
fluxctl sync
k apply -f chart-museum/
k apply -f kube-system/ 
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
./hack/vpa-up.sh
k rollout restart deployment ingress-external-nginx-ingress-controller
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
fluxctl sync
k apply -f kube-system/ 
k apply -f kube-system/ 
helm repo add eksctlcharts http://2807a9f0-chartmuseum-chart-310c-1472363394.us-east-1.elb.amazonaws.com:8080 --password "***REMOVED***" --username chart-user --insecure-skip-tls-verify 
kdelhr unleash
k apply -f unleash/
fluxctl sync
kdelhr unleash
k apply -f unleash/
fluxctl sync
kdelhr unleash
k apply -f unleash/
k apply -f unleash/
eksctl create iamserviceaccount --config-file ./cluster/cluster.yaml
java -jar ~/workspace/bfg-repo-cleaner/bfg/target/bfg-1.13.0-tags/v1.13.0-2c1ec2f.jar  -rt  secret.txt
fluxctl sync
fluxctl sync
kl -f -n flux flux-5944c85f5b-qqzrb
java -jar ~/workspace/bfg-repo-cleaner/bfg/target/bfg-1.13.0-tags/v1.13.0-2c1ec2f.jar  -rt  secret.txt
fluxctl sync
k rollout restart deployment ingress-external-nginx-ingress-controller
git reflog expire --expire=now --all && git gc --prune=now --aggressive
k rollout restart deployment ingress-external-nginx-ingress-controller
fluxctl sync
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ***REMOVED***.dkr.ecr.us-east-1.amazonaws.com
k apply -f unleash/
k rollout restart deployment secret-manager-kubernetes-external-secrets
kcn unleash
fluxctl sync
fluxctl sync
kl -f -n kube-system ingress-external-nginx-ingress-controller-b4784b9c-pgs52
fluxctl sync
fluxctl sync
fluxctl sync
k rollout restart deployment unleash
fluxctl sync
fluxctl sync
k apply -f monitoring/
k get podsecuritypolicies.policy | awk '{print $1}' | grep -iv eks  | grep -v NAME | xargs kubectl delete podsecuritypolicy
kcn monitoring
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f monitoring/
k apply -f ingress-nginx
k apply -f ingress-nginx
k apply -f ingress-nginx
fluxctl sync
k scale deployment --replicas 0 -n flux helm-operator
k get clusterrole -A | grep prom | awk '{print $1}' | xargs kubectl delete clusterrole
k get clusterrole -A | grep prom | awk '{print $1}' | xargs kubectl delete clusterrole
k apply -f ingress-nginx/
k scale deployment --replicas 1 -n flux  flux
k scale deployment --replicas 1 -n flux  flux
k scale deployment --replicas 0 -n flux flux helm-operator
fluxctl sync
kdelp prom-operator-prometheus-node-exporter-6ljgh 
export FLUX_FORWARD_NAMESPACE=flux
k scale deployment --replicas 0 -n flux flux helm-operator
k get podsecuritypolicies.policy | awk '{print $1}' | grep -iv eks  | grep -v NAME | xargs kubectl delete podsecuritypolicy
k delete validatingwebhookconfigurations.admissionregistration.k8s.io prom-operator-prometheus-o-admission
k delete validatingwebhookconfigurations.admissionregistration.k8s.io prom-operator-prometheus-o-admission
k get mutatingwebhookconfigurations.admissionregistration.k8s.io
k scale deployment --replicas 1 -n flux flux helm-operator
fluxctl sync
export FLUX_FORWARD_NAMESPACE=flux
k apply -f ingress-nginx/
k apply -f monitoring/
kdelhr prom-operator
k apply -f monitoring/
k get mutatingwebhookconfigurations.admissionregistration.k8s.io | grep prom | awk '{print $1}'  | xargs kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io prom-operator-prometheus-o-admission
k apply -f monitoring/
k get validatingwebhookconfigurations.admissionregistration.k8s.io | grep grafana | awk '{print $1}'  | xargs kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io
k get validatingwebhookconfigurations.admissionregistration.k8s.io | grep grafana | awk '{print $1}'  | xargs kubectl delete v
k get mutatingwebhookconfigurations.admissionregistration.k8s.io | grep grafana | awk '{print $1}'  | xargs kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io 
k get mutatingwebhookconfigurations.admissionregistration.k8s.io | grep prom | awk '{print $1}'  | xargs kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io prom-operator-prometheus-o-admission
k delete validatingwebhookconfigurations.admissionregistration.k8s.io prom-operator-prometheus-o-admission
k get mutatingwebhookconfigurations.admissionregistration.k8s.io | grep grafana | awk '{print $1}'  | xargs kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io 
k get podsecuritypolicies.policy | awk '{print $1}' | grep -iv eks  | grep -v NAME | xargs kubectl delete podsecuritypolicy
k get clusterrolebindings.rbac.authorization.k8s.io -A | grep ingres | awk '{print $1}' | xargs kubectl delete clusterrolebindings.rbac.authorization.k8s.io
k scale deployment --replicas 1 -n flux flux helm-operator
fluxctl sync
kl -f -n flux helm-operator-6fd6b7fbfc-r2pw2
fluxctl sync
fluxctl sync
fluxctl sync
kubectl run -i --tty ubuntu --image=ubuntu
k apply -f flux/
k apply -f flux/
k exec -it ubuntu-54ccfdd5bd-5gxhd zsh
k exec -it ubuntu-54ccfdd5bd-5gxhd zsh
java -jar ~/workspace/bfg-repo-cleaner/bfg/target/bfg-1.13.0-tags/v1.13.0-2c1ec2f.jar  -rt  secret.txt
assume-role.sh default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 830957\
: 1590802808:0;assume-role.sh default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 830957
assume-role.sh default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 861301
eksctl create cluster --config-file cluster.yaml
idea assume-role.sh 
kubectl config use-context STAGING
zshreload
zshreload
sudo idea /usr/local/bin/assume-role
assume-role.sh default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 861301
zshreload
assume-role default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 276408
assume-role default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 883025
assume-role default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 637915
aws eks update-kubeconfig --name ***REMOVED***infra-cluster-stg --role-arn arn:aws:iam::***REMOVED***:role/DevHubInfra-STG-DevHubDevK8sMaster8EEDE817-1CF8CE1FHFX5A
AWS_PROFILE=assume-role
assume-role default arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 896117
aws eks list-clusters
eksctl create cluster --config-file cluster.yaml
kubectl config use-context ***REMOVED***@flux-stack.us-east-1.eksctl.io
eksctl create cluster --config-file cluster.yaml
ohmyzsh
sudo idea /usr/local/bin/assume-role
idea >/dev/null 2>&1 &
kubectl config use-context ***REMOVED***@flux-stack.us-east-1.eksctl.io
kubectl config use-context STAGING
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
eksctl delete cluster --config-file ./cluster/cluster.yaml
assume-role ***REMOVED*** arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 548684
kubectl config use-context STAGING
zshreload
***REMOVED***
eksctl create cluster --config-file cluster.yaml
eksctl delete cluster --config-file cluster.yaml
aws eks list-clusters
idea >/dev/null 2>&1 &
sudo idea /usr/local/bin/assume-role
idea assume-role
zshreload
ohmyzsh
zshreload
upgrade_oh_my_zsh
code ~/.config/starship.toml
zshreload
zshreload
assume-role ***REMOVED*** arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 379371
eksctl create cluster --config-file cluster.yaml
assume-role ***REMOVED*** arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 868810
eksctl create cluster --config-file cluster.yaml
zshreload
git rebase HEAD~1 -i
git rebase HEAD~3 -i
idea ~/.config/starship.toml
kubectl config use-context STAGING
kubectl config use-context ***REMOVED***@flux-helm-op.us-east-1.eksctl.io
assume-role ***REMOVED*** arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 863956
assume-role ***REMOVED*** arn:aws:iam::***REMOVED***:mfa/***REMOVED*** 175030
zshreload
idea ~/.kube/config
asp assume-role
kubectl config use-context STAGING
idea ~/.config/starship.toml
idea ~/.config/starship.toml
zshreload 
aws eks update-kubeconfig --name javier
kubectl config use-context STAGING
